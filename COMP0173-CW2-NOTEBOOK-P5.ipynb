{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c925ac8a",
   "metadata": {},
   "source": [
    "# COMP0173: Coursework 2\n",
    "\n",
    "The paper HEARTS: A Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection by Theo King, Zekun Wu et al. (2024) presents a comprehensive approach to analysing and detecting stereotypes in text [1]. The authors introduce the HEARTS framework, which integrates model explainability, carbon-efficient training, and accurate evaluation across multiple bias-sensitive datasets. By using transformer-based models such as ALBERT-V2, BERT, and DistilBERT, this research project demonstrates that stereotype detection performance varies significantly across dataset sources, underlining the need for diverse evaluation benchmarks. The paper provides publicly available datasets and code [2], allowing full reproducibility and offering a standardised methodology for future research on bias and stereotype detection in Natural Language Processing (NLP).\n",
    "\n",
    "While the HEARTS framework evaluates stereotype detection in English, this project adapts the methodology to the Russian context. Russian stereotypes often rely on grammatical gender, morphology, and culture specific tropes. Although Russian is not classified as a low-resource language and many high-performing NLP models are available, there is currently no publicly accessible model specifically designed to detect stereotypes in Russian language. Existing models detecting toxicity or sentiment identify stereotypical and biased sentences only when they include specific patterns, such as insults, slurs, or identity-specific hate speech [8]. \n",
    "\n",
    "To address this gap, I introduce two fine-tuned classifiers, `AI-Forever-RuBert` [10] and `XML-RoBERTa` [11] trained on datasets `RBSA`, and `RBS`, respectively. Understanding these patterns is essential for applications such as content moderation, ensuring the safety of Russian-language LLMs, and monitoring harmful narratives across demographic groups and underrepresented societies. Adapting the HEARTS framework to this new sociolinguistic context illustrates its transferability beyond the English-speaking context and enables a more culturally grounded approach to bias detection, thereby promoting SDG 5: Gender Equality, SDG 10: Reduced Inequalities, and SDG 16: Peace, Justice, and Strong Institutions [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068fd33",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "All figures produced during this notebook are stored in the project’s `COMP0173_Figures` directory.\n",
    "The corresponding LaTeX-formatted performance comparison tables, jupyter notebooks are stored in `/COMP0173_PDF`. \n",
    "The compiled document are available as `COMP0173-CW2-TABLES.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-XX.pdf`.\n",
    "All prompts used for data augmentation are stored in `COMP0173_Prompts` and the manually collected stereotypes (with English translations) are provided in `COMP0173_Stereotypes`. \n",
    "The datasets used for model training and evaluation are stored in `COMP0173_Data` which contains: \n",
    "\n",
    "- rubias.tsv — RuBias dataset [6, 7]\n",
    "- ruster.csv — RuSter dataset (see Part 2 of the notebook for source websites)\n",
    "- rubist.csv — RBS dataset: RuBias + RuSter augmented with LLM-generated samples (Claude Sonnet), using a zero-shot prompt with examples\n",
    "- rubist_second.csv — RBSA dataset: RuBias + RuSter augmented with LLM-generated samples using a second prompt version without examples\n",
    "\n",
    "The notebooks `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P3.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P5.pdf` are replications of `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P2.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P4.pdf`, where P2 provides the new `RBSA` with second prompt (without examples) and P5 demonstrates the model running ON GPU (the results saved are from GPU fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279720f4",
   "metadata": {},
   "source": [
    "# Technical Implementation (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e9b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install -r requirements.txt\n",
    "# pip install transformers\n",
    "# pip install --upgrade transformers\n",
    "# pip install --upgrade tokenizers\n",
    "# pip install -U sentence-transformers\n",
    "# pip install natasha\n",
    "# pip install datasets\n",
    "# pip install --user -U nltk\n",
    "# conda install -c anaconda nltk\n",
    "# pip install --upgrade openai pandas tqdm\n",
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41c26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_trf\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download ru_core_news_lg\n",
    "\n",
    "# # GPU\n",
    "# pip install -U 'spacy[cuda12x]'\n",
    "# # GPU - Train Models\n",
    "# pip install -U 'spacy[cuda12x,transformers,lookups]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4495b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import random, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"pkg_resources is deprecated as an API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526aa252-67bc-4800-a639-661ef2b90b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib.util, pathlib\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "from importlib import reload\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import difflib\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b7bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, XLMWithLMHeadModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import platform\n",
    "from datasets import Dataset\n",
    "# import spacy \n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"Exploratory Data Analysis\")\n",
    "sys.path.append(\"Model Training and Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b794b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU host (UCL access)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# # Path\n",
    "# import os\n",
    "# os.chdir(\"/tmp/HEARTS-Text-Stereotype-Detection\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf84d7",
   "metadata": {},
   "source": [
    "## Part 4: Adapt the model architecture and training pipeline to your local context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f9404",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Justify architectural modifications for new context\n",
    "\n",
    "To adapt the HEARTS framework to the Russian context, I kept the original fine-tuning pipeline while substituting ALBERT-V2, BERT, and DistilBERT with encoder models optimised for Russian text. The specific models I fine-tuned include: \n",
    "\n",
    "- DeepPavlov/RuBERT [9]\n",
    "- AI-Forever/RuBERT [10]\n",
    "- XLM-RoBERTa (multilingual) [11]\n",
    "- Logistic Regression baselines using TF-IDF and SpaCy embeddings. \n",
    "\n",
    "Each model was configured as a binary stereotype classifier and trained separately on the RBS and RBSA datasets using the Hugging Face AutoModelForSequenceClassification architecture, with an 80/20 train-test split. To support sustainability goals, CodeCarbon was integrated into the pipeline to monitor emissions during fine-tuning. All models finished training in under 10 minutes per dataset, with total estimated emissions of less than 2 grams of CO₂ for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5dd0a1",
   "metadata": {},
   "source": [
    "![Hyperparameters](COMP0173_Figures/hyperparameters.png)\n",
    "\n",
    "![Model Configuration](COMP0173_Figures/configuration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final version \n",
    "rubist = pd.read_csv(\"COMP0173_Data/rubist.csv\", encoding=\"utf-8\")\n",
    "rubist_second = pd.read_csv(\"COMP0173_Data/rubist_second.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d65e7",
   "metadata": {},
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cee5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/hf\"\n",
    "os.makedirs(\"/tmp/hf\", exist_ok=True)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78bba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "     stereotype_type                                               text  \\\n",
      "168       profession              футболисты топ клубов следят за модой   \n",
      "2883     nationality  британцы танцуют с радужными пельменями на кры...   \n",
      "2919           lgbtq  бисексуалы на самом деле просто сексуально рас...   \n",
      "3412      profession  программирование доступно людям с разным уровн...   \n",
      "732            lgbtq        пансексуалы просто до конца не определились   \n",
      "\n",
      "      category data_name  \n",
      "168          1    rubist  \n",
      "2883         0    rubist  \n",
      "2919         1    rubist  \n",
      "3412         0    rubist  \n",
      "732          1    rubist  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "2726      profession  бедные люди питаются звездным светом и варят с...   \n",
      "1066          gender  женщины водят машины распевая серенады с говор...   \n",
      "2547     nationality  евреи как и представители других национальност...   \n",
      "1834      profession  бедный человек должен танцевать с радужными но...   \n",
      "1825     nationality  евреи как и представители других национальност...   \n",
      "\n",
      "      category data_name  \n",
      "2726         0    rubist  \n",
      "1066         0    rubist  \n",
      "2547         0    rubist  \n",
      "1834         0    rubist  \n",
      "1825         0    rubist  \n",
      "Train data size: 3372\n",
      "Test data size: 844\n",
      "First few examples from the training data:\n",
      "     stereotype_type                                           text  category  \\\n",
      "1005          gender                     мужчинам нужен только секс         1   \n",
      "1001          gender           мужчина рисует на работу каждый день         0   \n",
      "2619     nationality               французы рисуют очень много сыра         0   \n",
      "2213          gender  женщина успешна независимо от наличия мужчины         0   \n",
      "934           gender    мужчина партнер семейства а женщина партнер         0   \n",
      "\n",
      "          data_name  \n",
      "1005  rubist_second  \n",
      "1001  rubist_second  \n",
      "2619  rubist_second  \n",
      "2213  rubist_second  \n",
      "934   rubist_second  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "80        profession             чтобы быть музыкантом нужно иметь шкаф   \n",
      "1814          gender                    все девочки пылесосят мальчиков   \n",
      "2277      profession  эйчар это человек который не взяли больше нику...   \n",
      "61        profession  чтобы стать музыкантом можно в любом возрасте ...   \n",
      "755           gender  мужчина представитель пушистого пола главная о...   \n",
      "\n",
      "      category      data_name  \n",
      "80           0  rubist_second  \n",
      "1814         0  rubist_second  \n",
      "2277         0  rubist_second  \n",
      "61           0  rubist_second  \n",
      "755          0  rubist_second  \n",
      "Train data size: 2336\n",
      "Test data size: 584\n",
      "Number of unique labels: 2\n",
      "Testing C=0.01, penalty=l1 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.9466666666666667\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.6326957285464712\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.6120484633882092\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.9466666666666667\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.8929148318366249\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.8824924844557498\n",
      "Testing C=1, penalty=None => F1 Score: 0.9466666666666667\n",
      "Best model parameters: {'C': 0.01, 'penalty': None}\n",
      "Model and vectorizer saved to model_output_LR_tfidf/rubist_trained\n",
      "Estimated total emissions: 1.3855943050266042e-05 kg CO2\n",
      "Number of unique labels: 2\n",
      "Number of unique labels: 2\n",
      "Testing C=0.01, penalty=l1 => F1 Score: 0.4\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.4\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.5407413941385317\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.4\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.4\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.5407413941385317\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.46003885576472164\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.42689732142857145\n",
      "Testing C=1, penalty=None => F1 Score: 0.5407413941385317\n",
      "Best model parameters: {'C': 0.01, 'penalty': None}\n",
      "Model and vectorizer saved to model_output_LR_tfidf/rubist_second_trained\n",
      "Estimated total emissions: 1.2133621719059336e-05 kg CO2\n",
      "Number of unique labels: 2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 3372/3372 [00:33<00:00, 100.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing C=0.01, penalty=l1 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.8377403846153846\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.9196396682408032\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.8833912566306933\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.894211324570273\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.9196396682408032\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.9212241604072258\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.9172978203631145\n",
      "Testing C=1, penalty=None => F1 Score: 0.9196396682408032\n",
      "Best model parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Model and vectorizer saved to model_output_LR_embedding/rubist_trained\n",
      "Estimated total emissions: 0.0001950465552109412 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 844/844 [00:08<00:00, 100.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 2336/2336 [00:20<00:00, 112.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing C=0.01, penalty=l1 => F1 Score: 0.4\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.4068829055705911\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.627725258253562\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.4052597071464996\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.5424300867888139\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.627725258253562\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.6112426035502958\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.6005917159763313\n",
      "Testing C=1, penalty=None => F1 Score: 0.627725258253562\n",
      "Best model parameters: {'C': 0.01, 'penalty': None}\n",
      "Model and vectorizer saved to model_output_LR_embedding/rubist_second_trained\n",
      "Estimated total emissions: 0.00013900726188074115 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 584/584 [00:05<00:00, 112.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.763547</td>\n",
       "      <td>0.796915</td>\n",
       "      <td>0.779874</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.556180</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.530831</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.700342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.659863</td>\n",
       "      <td>0.652304</td>\n",
       "      <td>0.655353</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.694306</td>\n",
       "      <td>0.700342</td>\n",
       "      <td>0.696718</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.763547  0.796915  0.779874  389.000000\n",
       "1              0.556180  0.507692  0.530831  195.000000\n",
       "accuracy       0.700342  0.700342  0.700342    0.700342\n",
       "macro avg      0.659863  0.652304  0.655353  584.000000\n",
       "weighted avg   0.694306  0.700342  0.696718  584.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Logistic_Regression_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "\n",
    "# Execute full pipeline for logistic regression tfidf model\n",
    "train_model(train_data_rubist, model_output_base_dir='model_output_LR_tfidf', dataset_name='rubist_trained', feature_type='tfidf', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_LR_tfidf/rubist_trained', result_output_base_dir='result_output_LR_tfidf', dataset_name='rubist', feature_type='tfidf', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_output_base_dir='model_output_LR_tfidf', dataset_name='rubist_second_trained', feature_type='tfidf', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_LR_tfidf/rubist_second_trained', result_output_base_dir='result_output_LR_tfidf', dataset_name='rubist_second', feature_type='tfidf', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Execute full pipeline for logistic regression embedding model\n",
    "train_model(train_data_rubist, model_output_base_dir='model_output_LR_embedding', dataset_name='rubist_trained', feature_type='embedding', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_LR_embedding/rubist_trained', result_output_base_dir='result_output_LR_embedding', dataset_name='rubist', feature_type='embedding', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_output_base_dir='model_output_LR_embedding', dataset_name='rubist_second_trained', feature_type='embedding', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_LR_embedding/rubist_second_trained', result_output_base_dir='result_output_LR_embedding', dataset_name='rubist_second', feature_type='embedding', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b628611",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Document hyperparameter tuning process - GPU\n",
    "\n",
    "Hyperparameter tuning followed the structure of the original HEARTS pipeline but was adapted to Russian-language models and the two augmented datasets (RBS and RBSA). All experiments were run on a GPU-enabled environment to support efficient fine-tuning of transformer models. Before each run, GPU memory was cleared using:\n",
    "\n",
    "`gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "`\n",
    "\n",
    "The tuning process began by loading the two datasets (rubist.csv and rubist_second.csv) using the customised data_loader() function. For each model, a consistent training configuration was used to enable fair comparison. Unfortunately due to disk quota, it was not possible to run the `XLM-Roberta` model on GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fedad4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "     stereotype_type                                               text  \\\n",
      "168       profession              футболисты топ клубов следят за модой   \n",
      "2883     nationality  британцы танцуют с радужными пельменями на кры...   \n",
      "2919           lgbtq  бисексуалы на самом деле просто сексуально рас...   \n",
      "3412      profession  программирование доступно людям с разным уровн...   \n",
      "732            lgbtq        пансексуалы просто до конца не определились   \n",
      "\n",
      "      category data_name  \n",
      "168          1    rubist  \n",
      "2883         0    rubist  \n",
      "2919         1    rubist  \n",
      "3412         0    rubist  \n",
      "732          1    rubist  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "2726      profession  бедные люди питаются звездным светом и варят с...   \n",
      "1066          gender  женщины водят машины распевая серенады с говор...   \n",
      "2547     nationality  евреи как и представители других национальност...   \n",
      "1834      profession  бедный человек должен танцевать с радужными но...   \n",
      "1825     nationality  евреи как и представители других национальност...   \n",
      "\n",
      "      category data_name  \n",
      "2726         0    rubist  \n",
      "1066         0    rubist  \n",
      "2547         0    rubist  \n",
      "1834         0    rubist  \n",
      "1825         0    rubist  \n",
      "Train data size: 3372\n",
      "Test data size: 844\n",
      "First few examples from the training data:\n",
      "     stereotype_type                                           text  category  \\\n",
      "1005          gender                     мужчинам нужен только секс         1   \n",
      "1001          gender           мужчина рисует на работу каждый день         0   \n",
      "2619     nationality               французы рисуют очень много сыра         0   \n",
      "2213          gender  женщина успешна независимо от наличия мужчины         0   \n",
      "934           gender    мужчина партнер семейства а женщина партнер         0   \n",
      "\n",
      "          data_name  \n",
      "1005  rubist_second  \n",
      "1001  rubist_second  \n",
      "2619  rubist_second  \n",
      "2213  rubist_second  \n",
      "934   rubist_second  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "80        profession             чтобы быть музыкантом нужно иметь шкаф   \n",
      "1814          gender                    все девочки пылесосят мальчиков   \n",
      "2277      profession  эйчар это человек который не взяли больше нику...   \n",
      "61        profession  чтобы стать музыкантом можно в любом возрасте ...   \n",
      "755           gender  мужчина представитель пушистого пола главная о...   \n",
      "\n",
      "      category      data_name  \n",
      "80           0  rubist_second  \n",
      "1814         0  rubist_second  \n",
      "2277         0  rubist_second  \n",
      "61           0  rubist_second  \n",
      "755          0  rubist_second  \n",
      "Train data size: 2336\n",
      "Test data size: 584\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Attempting to convert .bin model on the fly to safetensors.\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8b4541b76a40fb8ebe4ea14a0dcd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78de93ce883744f7b7ae8af444809e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [101, 12528, 23558, 29466, 12938, 21264, 57041, 869, 1516, 33165, 27585, 9210, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f396908956e4b51a1bde8b6055582dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad65040f97d45a38fa918b8ae4eacc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [101, 12528, 23558, 29466, 12938, 21264, 57041, 869, 1516, 33165, 27585, 9210, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,697\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258\n",
      "  Number of trainable parameters = 177,854,978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/258 02:15, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.148108</td>\n",
       "      <td>0.958421</td>\n",
       "      <td>0.929174</td>\n",
       "      <td>0.941696</td>\n",
       "      <td>0.929174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.071734</td>\n",
       "      <td>0.967716</td>\n",
       "      <td>0.965641</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.965641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.087904</td>\n",
       "      <td>0.970066</td>\n",
       "      <td>0.970066</td>\n",
       "      <td>0.970066</td>\n",
       "      <td>0.970066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>0.982161</td>\n",
       "      <td>0.977847</td>\n",
       "      <td>0.979955</td>\n",
       "      <td>0.977847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.121155</td>\n",
       "      <td>0.974393</td>\n",
       "      <td>0.962360</td>\n",
       "      <td>0.968007</td>\n",
       "      <td>0.962360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.123942</td>\n",
       "      <td>0.974393</td>\n",
       "      <td>0.962360</td>\n",
       "      <td>0.968007</td>\n",
       "      <td>0.962360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-43\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-43/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-43/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-43/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-86\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-86/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-86/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-86/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-86/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_trained/checkpoint-43] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-129\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-129/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-129/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-129/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-129/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-172\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-172/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-172/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-172/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-172/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_trained/checkpoint-86] due to args.save_total_limit\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_trained/checkpoint-129] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-215\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-215/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-215/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-215/special_tokens_map.json\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-258\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_trained/checkpoint-215] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained/checkpoint-258\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/checkpoint-258/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_deeppavlov_rubert/rubist_trained/checkpoint-172 (score: 0.0698399767279625).\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_trained/checkpoint-258] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_trained\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_trained/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_trained/special_tokens_map.json\n",
      "loading configuration file model_output_deeppavlov_rubert/rubist_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"model_output_deeppavlov_rubert/rubist_trained\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model_output_deeppavlov_rubert/rubist_trained/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0012824234634039179 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_deeppavlov_rubert/rubist_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6398b8bc23014cfa8a8344ac958bd4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0414d23d804b91871d45e810c53391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'stereotype_type': 'profession', 'text': 'бедные люди питаются звездным светом и варят суп из радуги', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 2726, 'input_ids': [101, 94818, 11894, 38663, 13904, 2010, 54564, 851, 84652, 868, 10508, 1703, 13158, 23726, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Attempting to convert .bin model on the fly to safetensors.\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/snapshots/4036cab694767a299f2b9e6492909664d9414229/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0292ca5d789d42989565f3b61470a975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4788e483de3540dca988162d405986b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'stereotype_type': 'nationality', 'text': 'все эстонцы очень пушистые', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 1567, 'input_ids': [101, 4752, 92660, 4402, 7805, 14741, 23939, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc7ae14c3684efeb38898d4ad5679f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6576bd76ee40e58b267f2456efff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'stereotype_type': 'nationality', 'text': 'все эстонцы очень пушистые', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 1567, 'input_ids': [101, 4752, 92660, 4402, 7805, 14741, 23939, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,868\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n",
      "  Number of trainable parameters = 177,854,978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 02:08, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.646596</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.644738</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.556053</td>\n",
       "      <td>0.685612</td>\n",
       "      <td>0.699511</td>\n",
       "      <td>0.685977</td>\n",
       "      <td>0.699511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.475200</td>\n",
       "      <td>0.486084</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.739002</td>\n",
       "      <td>0.748674</td>\n",
       "      <td>0.739002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.478719</td>\n",
       "      <td>0.767994</td>\n",
       "      <td>0.748392</td>\n",
       "      <td>0.755586</td>\n",
       "      <td>0.748392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.342600</td>\n",
       "      <td>0.478887</td>\n",
       "      <td>0.774481</td>\n",
       "      <td>0.750450</td>\n",
       "      <td>0.758837</td>\n",
       "      <td>0.750450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-30] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-60] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-90] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-120] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-150 (score: 0.47871941328048706).\n",
      "Deleting older checkpoint [model_output_deeppavlov_rubert/rubist_second_trained/checkpoint-180] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_deeppavlov_rubert/rubist_second_trained\n",
      "Configuration saved in model_output_deeppavlov_rubert/rubist_second_trained/config.json\n",
      "Model weights saved in model_output_deeppavlov_rubert/rubist_second_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_deeppavlov_rubert/rubist_second_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_deeppavlov_rubert/rubist_second_trained/special_tokens_map.json\n",
      "loading configuration file model_output_deeppavlov_rubert/rubist_second_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"model_output_deeppavlov_rubert/rubist_second_trained\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model_output_deeppavlov_rubert/rubist_second_trained/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.001094457420633383 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_deeppavlov_rubert/rubist_second_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e472aeaf0c54a8d96158a68bbf1f78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b39a4f472d74221a21606e476994d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'stereotype_type': 'profession', 'text': 'чтобы быть музыкантом нужно иметь шкаф', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 80, 'input_ids': [101, 5247, 6345, 44670, 15411, 16038, 74989, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.835476</td>\n",
       "      <td>0.821745</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.625995</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.758562</td>\n",
       "      <td>0.758562</td>\n",
       "      <td>0.758562</td>\n",
       "      <td>0.758562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.728405</td>\n",
       "      <td>0.720302</td>\n",
       "      <td>0.723870</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.754998</td>\n",
       "      <td>0.758562</td>\n",
       "      <td>0.756383</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.808458  0.835476  0.821745  389.000000\n",
       "1              0.648352  0.605128  0.625995  195.000000\n",
       "accuracy       0.758562  0.758562  0.758562    0.758562\n",
       "macro avg      0.728405  0.720302  0.723870  584.000000\n",
       "weighted avg   0.754998  0.758562  0.756383  584.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='DeepPavlov/rubert-base-cased', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_deeppavlov_rubert', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_deeppavlov_rubert/rubist_trained', result_output_base_dir='result_output_deeppavlov_rubert', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='DeepPavlov/rubert-base-cased', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_deeppavlov_rubert', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_deeppavlov_rubert/rubist_second_trained', result_output_base_dir='result_output_deeppavlov_rubert', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef84fce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "     stereotype_type                                               text  \\\n",
      "168       profession              футболисты топ клубов следят за модой   \n",
      "2883     nationality  британцы танцуют с радужными пельменями на кры...   \n",
      "2919           lgbtq  бисексуалы на самом деле просто сексуально рас...   \n",
      "3412      profession  программирование доступно людям с разным уровн...   \n",
      "732            lgbtq        пансексуалы просто до конца не определились   \n",
      "\n",
      "      category data_name  \n",
      "168          1    rubist  \n",
      "2883         0    rubist  \n",
      "2919         1    rubist  \n",
      "3412         0    rubist  \n",
      "732          1    rubist  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "2726      profession  бедные люди питаются звездным светом и варят с...   \n",
      "1066          gender  женщины водят машины распевая серенады с говор...   \n",
      "2547     nationality  евреи как и представители других национальност...   \n",
      "1834      profession  бедный человек должен танцевать с радужными но...   \n",
      "1825     nationality  евреи как и представители других национальност...   \n",
      "\n",
      "      category data_name  \n",
      "2726         0    rubist  \n",
      "1066         0    rubist  \n",
      "2547         0    rubist  \n",
      "1834         0    rubist  \n",
      "1825         0    rubist  \n",
      "Train data size: 3372\n",
      "Test data size: 844\n",
      "First few examples from the training data:\n",
      "     stereotype_type                                           text  category  \\\n",
      "1005          gender                     мужчинам нужен только секс         1   \n",
      "1001          gender           мужчина рисует на работу каждый день         0   \n",
      "2619     nationality               французы рисуют очень много сыра         0   \n",
      "2213          gender  женщина успешна независимо от наличия мужчины         0   \n",
      "934           gender    мужчина партнер семейства а женщина партнер         0   \n",
      "\n",
      "          data_name  \n",
      "1005  rubist_second  \n",
      "1001  rubist_second  \n",
      "2619  rubist_second  \n",
      "2213  rubist_second  \n",
      "934   rubist_second  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "80        profession             чтобы быть музыкантом нужно иметь шкаф   \n",
      "1814          gender                    все девочки пылесосят мальчиков   \n",
      "2277      profession  эйчар это человек который не взяли больше нику...   \n",
      "61        profession  чтобы стать музыкантом можно в любом возрасте ...   \n",
      "755           gender  мужчина представитель пушистого пола главная о...   \n",
      "\n",
      "      category      data_name  \n",
      "80           0  rubist_second  \n",
      "1814         0  rubist_second  \n",
      "2277         0  rubist_second  \n",
      "61           0  rubist_second  \n",
      "755          0  rubist_second  \n",
      "Train data size: 2336\n",
      "Test data size: 584\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4c735577bd4e7cb753c04cf00c904f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/590 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121fc41eb1ca485499d16ad6f70da621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/716M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Attempting to convert .bin model on the fly to safetensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f9a191a98d419eae5e53f0ccfc8cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/716M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec549ae009748ebb66d12c35450cced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6468dd1295422bb2393041b8e84d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b45a41d5fc486d82a5c25e13458750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [101, 4269, 12924, 15663, 3822, 16375, 52066, 699, 110, 102821, 5897, 921, 82766, 1306, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b41715b37fa4737bfa0de480b1b1a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25799cd75504455da7e7f08ee1447ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [101, 4269, 12924, 15663, 3822, 16375, 52066, 699, 110, 102821, 5897, 921, 82766, 1306, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,697\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258\n",
      "  Number of trainable parameters = 178,308,866\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/258 02:17, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.957973</td>\n",
       "      <td>0.958975</td>\n",
       "      <td>0.958471</td>\n",
       "      <td>0.958975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>0.972724</td>\n",
       "      <td>0.977802</td>\n",
       "      <td>0.975190</td>\n",
       "      <td>0.977802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.088004</td>\n",
       "      <td>0.973177</td>\n",
       "      <td>0.966770</td>\n",
       "      <td>0.969865</td>\n",
       "      <td>0.966770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.973392</td>\n",
       "      <td>0.973392</td>\n",
       "      <td>0.973392</td>\n",
       "      <td>0.973392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.097585</td>\n",
       "      <td>0.974311</td>\n",
       "      <td>0.968982</td>\n",
       "      <td>0.971571</td>\n",
       "      <td>0.968982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.096686</td>\n",
       "      <td>0.975449</td>\n",
       "      <td>0.971195</td>\n",
       "      <td>0.973274</td>\n",
       "      <td>0.971195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-43\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-43/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-43/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-43/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-86\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-86/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-86/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-86/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-86/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_trained/checkpoint-43] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-129\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-129/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-129/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-129/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-129/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-172\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-172/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-172/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-172/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-172/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_trained/checkpoint-129] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-215\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-215/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-215/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-215/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_trained/checkpoint-172] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-258\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-258/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-258/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-258/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-258/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_trained/checkpoint-215] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained/checkpoint-258\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/checkpoint-258/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/checkpoint-258/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/checkpoint-258/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/checkpoint-258/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_ruberta_base/rubist_trained/checkpoint-86 (score: 0.06719378381967545).\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_trained/checkpoint-258] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_trained\n",
      "Configuration saved in model_output_ruberta_base/rubist_trained/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_trained/special_tokens_map.json\n",
      "loading configuration file model_output_ruberta_base/rubist_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"model_output_ruberta_base/rubist_trained\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model_output_ruberta_base/rubist_trained/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0013583508792769448 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_ruberta_base/rubist_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb4f3e725894710a799fa2b13c51e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7b108148c40408c3ed3dffc5973c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'stereotype_type': 'profession', 'text': 'бедные люди питаются звездным светом и варят суп из радуги', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 2726, 'input_ids': [101, 42890, 1950, 47937, 10088, 815, 16465, 107, 84625, 18777, 734, 74394, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Attempting to convert .bin model on the fly to safetensors.\n",
      "Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--ai-forever--ruBert-base/snapshots/05f37a2ca9e333fd18f30cd0c96c68d274793c69/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ai-forever/ruBert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0225c07653334f2682b5721ded7f74ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d937c8838b4beaa1875aae8615527f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'stereotype_type': 'nationality', 'text': 'все эстонцы очень пушистые', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 1567, 'input_ids': [101, 780, 34330, 1048, 1179, 94517, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a8615ff995403b9286ccee82d1f3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4d1731a7c946d5b13671f5c8636316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'stereotype_type': 'nationality', 'text': 'все эстонцы очень пушистые', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 1567, 'input_ids': [101, 780, 34330, 1048, 1179, 94517, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,868\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n",
      "  Number of trainable parameters = 178,308,866\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 02:04, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.594298</td>\n",
       "      <td>0.729487</td>\n",
       "      <td>0.638153</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.638153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.457805</td>\n",
       "      <td>0.750443</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.746550</td>\n",
       "      <td>0.743504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316400</td>\n",
       "      <td>0.453952</td>\n",
       "      <td>0.777977</td>\n",
       "      <td>0.788011</td>\n",
       "      <td>0.781933</td>\n",
       "      <td>0.788011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.489346</td>\n",
       "      <td>0.782447</td>\n",
       "      <td>0.792642</td>\n",
       "      <td>0.786476</td>\n",
       "      <td>0.792642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.493280</td>\n",
       "      <td>0.799342</td>\n",
       "      <td>0.795343</td>\n",
       "      <td>0.797230</td>\n",
       "      <td>0.795343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.515137</td>\n",
       "      <td>0.798690</td>\n",
       "      <td>0.797916</td>\n",
       "      <td>0.798299</td>\n",
       "      <td>0.797916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-30\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-30/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-30/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-60\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-60/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-60/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-60/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_second_trained/checkpoint-30] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-90\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-90/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-90/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-90/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_second_trained/checkpoint-60] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-120\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-120/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-120/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-120/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-150\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-150/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_second_trained/checkpoint-120] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-180\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_second_trained/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 468\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained/checkpoint-180\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/checkpoint-180/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_ruberta_base/rubist_second_trained/checkpoint-90 (score: 0.45395180583000183).\n",
      "Deleting older checkpoint [model_output_ruberta_base/rubist_second_trained/checkpoint-180] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_ruberta_base/rubist_second_trained\n",
      "Configuration saved in model_output_ruberta_base/rubist_second_trained/config.json\n",
      "Model weights saved in model_output_ruberta_base/rubist_second_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_ruberta_base/rubist_second_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_ruberta_base/rubist_second_trained/special_tokens_map.json\n",
      "loading configuration file model_output_ruberta_base/rubist_second_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"model_output_ruberta_base/rubist_second_trained\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 120138\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model_output_ruberta_base/rubist_second_trained/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0010754149601394114 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at model_output_ruberta_base/rubist_second_trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baff2e7764974a5e848de81d33aa92a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd63aeeb9a14fbb96751e3876139bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'stereotype_type': 'profession', 'text': 'чтобы быть музыкантом нужно иметь шкаф', 'category': 0, 'data_name': 'rubist_second', '__index_level_0__': 80, 'input_ids': [101, 1015, 1202, 61810, 1885, 4821, 22860, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.848329</td>\n",
       "      <td>0.834387</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.675824</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.652520</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.775685</td>\n",
       "      <td>0.775685</td>\n",
       "      <td>0.775685</td>\n",
       "      <td>0.775685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.748360</td>\n",
       "      <td>0.739549</td>\n",
       "      <td>0.743453</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.772456</td>\n",
       "      <td>0.775685</td>\n",
       "      <td>0.773661</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.820896  0.848329  0.834387  389.000000\n",
       "1              0.675824  0.630769  0.652520  195.000000\n",
       "accuracy       0.775685  0.775685  0.775685    0.775685\n",
       "macro avg      0.748360  0.739549  0.743453  584.000000\n",
       "weighted avg   0.772456  0.775685  0.773661  584.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='ai-forever/ruBert-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_ruberta_base', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_ruberta_base/rubist_trained', result_output_base_dir='result_output_ruberta_base', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='ai-forever/ruBert-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_ruberta_base', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_ruberta_base/rubist_second_trained', result_output_base_dir='result_output_ruberta_base', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "     stereotype_type                                               text  \\\n",
      "168       profession              футболисты топ клубов следят за модой   \n",
      "2883     nationality  британцы танцуют с радужными пельменями на кры...   \n",
      "2919           lgbtq  бисексуалы на самом деле просто сексуально рас...   \n",
      "3412      profession  программирование доступно людям с разным уровн...   \n",
      "732            lgbtq        пансексуалы просто до конца не определились   \n",
      "\n",
      "      category data_name  \n",
      "168          1    rubist  \n",
      "2883         0    rubist  \n",
      "2919         1    rubist  \n",
      "3412         0    rubist  \n",
      "732          1    rubist  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "2726      profession  бедные люди питаются звездным светом и варят с...   \n",
      "1066          gender  женщины водят машины распевая серенады с говор...   \n",
      "2547     nationality  евреи как и представители других национальност...   \n",
      "1834      profession  бедный человек должен танцевать с радужными но...   \n",
      "1825     nationality  евреи как и представители других национальност...   \n",
      "\n",
      "      category data_name  \n",
      "2726         0    rubist  \n",
      "1066         0    rubist  \n",
      "2547         0    rubist  \n",
      "1834         0    rubist  \n",
      "1825         0    rubist  \n",
      "Train data size: 3372\n",
      "Test data size: 844\n",
      "First few examples from the training data:\n",
      "     stereotype_type                                           text  category  \\\n",
      "1005          gender                     мужчинам нужен только секс         1   \n",
      "1001          gender           мужчина рисует на работу каждый день         0   \n",
      "2619     nationality               французы рисуют очень много сыра         0   \n",
      "2213          gender  женщина успешна независимо от наличия мужчины         0   \n",
      "934           gender    мужчина партнер семейства а женщина партнер         0   \n",
      "\n",
      "          data_name  \n",
      "1005  rubist_second  \n",
      "1001  rubist_second  \n",
      "2619  rubist_second  \n",
      "2213  rubist_second  \n",
      "934   rubist_second  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "80        profession             чтобы быть музыкантом нужно иметь шкаф   \n",
      "1814          gender                    все девочки пылесосят мальчиков   \n",
      "2277      profession  эйчар это человек который не взяли больше нику...   \n",
      "61        profession  чтобы стать музыкантом можно в любом возрасте ...   \n",
      "755           gender  мужчина представитель пушистого пола главная о...   \n",
      "\n",
      "      category      data_name  \n",
      "80           0  rubist_second  \n",
      "1814         0  rubist_second  \n",
      "2277         0  rubist_second  \n",
      "61           0  rubist_second  \n",
      "755          0  rubist_second  \n",
      "Train data size: 2336\n",
      "Test data size: 584\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52dfc67014247d8bea48cb1cd099980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd62efea00a40759166ea4c089d6d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a39bb7e6064e20bf384eb03ec2fa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa205c6f45ff43bf89719045fdb610e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b23ce815234078b723b0882926dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--FacebookAI--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200f2baee5c046d8a20c1d241207c44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c0d92641a84ffc8b3ca804a692cfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [0, 81939, 440, 14276, 4684, 92354, 103, 19816, 2791, 42678, 174783, 4401, 135, 129, 104335, 55533, 86783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee8933fa48444a29529f48f7c7f0639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0337b1a165d04b079a5cf9fff405ce98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'stereotype_type': 'gender', 'text': 'женщины принимают важные решения консультируясь с поющими чайниками', 'category': 0, 'data_name': 'rubist', '__index_level_0__': 1317, 'input_ids': [0, 81939, 440, 14276, 4684, 92354, 103, 19816, 2791, 42678, 174783, 4401, 135, 129, 104335, 55533, 86783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,697\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258\n",
      "  Number of trainable parameters = 278,045,186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/258 00:19 < 01:38, 2.17 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.370748</td>\n",
       "      <td>0.900178</td>\n",
       "      <td>0.752212</td>\n",
       "      <td>0.779849</td>\n",
       "      <td>0.752212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: data_name, __index_level_0__, category, text, stereotype_type. If data_name, __index_level_0__, category, text, stereotype_type are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_xlm_roberta_base/rubist_trained/checkpoint-43\n",
      "Configuration saved in model_output_xlm_roberta_base/rubist_trained/checkpoint-43/config.json\n",
      "Model weights saved in model_output_xlm_roberta_base/rubist_trained/checkpoint-43/model.safetensors\n",
      "tokenizer config file saved in model_output_xlm_roberta_base/rubist_trained/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in model_output_xlm_roberta_base/rubist_trained/checkpoint-43/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='FacebookAI/xlm-roberta-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_xlm_roberta_base', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_xlm_roberta_base/rubist_trained', result_output_base_dir='result_output_xlm_roberta_base', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='FacebookAI/xlm-roberta-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_xlm_roberta_base', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_xlm_roberta_base/rubist_second_trained', result_output_base_dir='result_output_xlm_roberta_base', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53bd118",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[1] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS: A holistic framework for explainable, sustainable and robust text stereotype detection.\n",
    "arXiv preprint arXiv:2409.11579.\n",
    "Available at: https://arxiv.org/abs/2409.11579\n",
    "(Accessed: 4 December 2025).\n",
    "https://doi.org/10.48550/arXiv.2409.11579\n",
    "\n",
    "[2] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS-Text-Stereotype-Detection (GitHub Repository).\n",
    "Available at: https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[3] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. Holistic AI. 2024.\n",
    "EMGSD: Expanded Multi-Group Stereotype Dataset (HuggingFace Dataset).\n",
    "Available at: https://huggingface.co/datasets/holistic-ai/EMGSD\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[4] University College London Technical Support Group (TSG).\n",
    "2025. GPU Access and Usage Documentation.\n",
    "Available at: https://tsg.cs.ucl.ac.uk/gpus/\n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[5] United Nations. 2025. The 2030 Agenda for Sustainable Development. \n",
    "Available at: https://sdgs.un.org/2030agenda \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[6] Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, and Ekaterina Artemova. 2024.\n",
    "RuBia: A Russian Language Bias Detection Dataset.\n",
    "Available at: https://arxiv.org/abs/2403.17553\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[7] Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, and Ekaterina Artemova. 2024.\n",
    "RuBia-Dataset (GitHub Repository).\n",
    "Available at: https://github.com/vergrig/RuBia-Dataset\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[8] Sismetanin. 2020. Toxic Comments Detection in Russian (GitHub Repository).\n",
    "Available at: https://github.com/sismetanin/toxic-comments-detection-in-russian\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[9] DeepPavlov. 2019. RuBERT-base-cased (Hugging Face Model).\n",
    "Available at: https://huggingface.co/DeepPavlov/rubert-base-cased\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[10] AI-Forever. 2023. RuBERT-base (Hugging Face Model).\n",
    "Available at: https://huggingface.co/ai-forever/ruBert-base\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[11] Hugging Face. 2024. XLM-RoBERTa: Model Documentation.\n",
    "Available at: https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[12] DeepPavlov. 2020. ruBERT-base-cased-sentence (Hugging Face Model).\n",
    "Available at: https://huggingface.co/DeepPavlov/rubert-base-cased-sentence\n",
    "(Accessed: 9 December 2025)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
