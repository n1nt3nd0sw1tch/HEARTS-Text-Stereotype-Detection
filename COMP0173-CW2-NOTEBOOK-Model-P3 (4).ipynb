{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c925ac8a",
   "metadata": {},
   "source": [
    "# COMP0173: Coursework 2\n",
    "\n",
    "The paper HEARTS: A Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection by Theo King, Zekun Wu et al. (2024) presents a comprehensive approach to analysing and detecting stereotypes in text [1]. The authors introduce the HEARTS framework, which integrates model explainability, carbon-efficient training, and accurate evaluation across multiple bias-sensitive datasets. By using transformer-based models such as ALBERT-V2, BERT, and DistilBERT, this research project demonstrates that stereotype detection performance varies significantly across dataset sources, underlining the need for diverse evaluation benchmarks. The paper provides publicly available datasets and code [2], allowing full reproducibility and offering a standardised methodology for future research on bias and stereotype detection in Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068fd33",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "All figures produced during this notebook are stored in the project’s `/COMP0173_Figures` directory.\n",
    "The corresponding LaTeX-formatted performance comparison tables, including ALBERT-V2, BERT, and DistilBERT are stored in `/COMP0173_PDF`, with the compiled document available as `COMP0173-CW2-TABLES.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279720f4",
   "metadata": {},
   "source": [
    "# Technical Implementation (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e9b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install -r requirements.txt\n",
    "# pip install transformers\n",
    "# pip install --upgrade transformers\n",
    "# pip install --upgrade tokenizers\n",
    "# pip install -U sentence-transformers\n",
    "# pip install natasha\n",
    "# pip install datasets\n",
    "# pip install --user -U nltk\n",
    "# conda install -c anaconda nltk\n",
    "# pip install --upgrade openai pandas tqdm\n",
    "# pip install dotenv\n",
    "# python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41c26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_trf\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download ru_core_news_lg\n",
    "\n",
    "# # GPU\n",
    "# pip install -U 'spacy[cuda12x]'\n",
    "# # GPU - Train Models\n",
    "# pip install -U 'spacy[cuda12x,transformers,lookups]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4495b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import random, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"pkg_resources is deprecated as an API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526aa252-67bc-4800-a639-661ef2b90b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib.util, pathlib\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "from importlib import reload\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import difflib\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b7bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, XLMWithLMHeadModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import platform\n",
    "from datasets import Dataset\n",
    "# import spacy \n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"Exploratory Data Analysis\")\n",
    "sys.path.append(\"Model Training and Evaluation\")\n",
    "\n",
    "from Initial_EDA import (\n",
    "    prepare_target_variable_distribution,\n",
    "    prepare_group_distribution,\n",
    "    prepare_text_length_analysis,\n",
    "    create_word_cloud\n",
    ")\n",
    "\n",
    "from Sentiment_Toxicity_Analysis_Ru import analyse_sentiment_and_regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b794b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU host (UCL access)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# # Path\n",
    "# import os\n",
    "# os.chdir(\"/tmp/HEARTS-Text-Stereotype-Detection\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf84d7",
   "metadata": {},
   "source": [
    "## Part 4: Adapt the model architecture and training pipeline to your local context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b04a90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e6e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart_domain(df, column, name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot the percentage distribution of social-group domains as a styled pie chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing a categorical column representing social domains.\n",
    "    column : str, optional\n",
    "        Name of the column in `df` holding domain labels. \n",
    "        \n",
    "    column : str, optional\n",
    "        Name of the dataset. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a pie chart visualising the proportional distribution of categories.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The function applies a custom colour palette tailored for the RuBias dataset \n",
    "    (gender, class, nationality, LGBTQ). Any unseen categories default to grey.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute relative frequency (%) of categories\n",
    "    domain_counts = df[column].value_counts(normalize=True) * 100\n",
    "    labels = domain_counts.index\n",
    "    sizes = domain_counts.values\n",
    "\n",
    "    # Predefined colour palette\n",
    "    color_map = {\n",
    "        'gender':      \"#CA5353\",  \n",
    "        'profession':  \"#F1A72F\",  \n",
    "        'nationality': \"#559A67\",  \n",
    "        'lgbtq':       \"#527BCD\",  \n",
    "    }\n",
    "    # Assign colours; fallback to grey for unknown labels\n",
    "    colors = [color_map.get(lbl, 'grey') for lbl in labels]\n",
    "\n",
    "    # Create compact, high-resolution figure\n",
    "    plt.figure(figsize=(5.5, 4), dpi=155)\n",
    "\n",
    "    # Draw pie chart with formatted percentages\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        sizes,\n",
    "        labels=None,\n",
    "        autopct='%1.1f%%',\n",
    "        pctdistance=0.55,\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        wedgeprops={'linewidth': 2, 'edgecolor': 'white'}\n",
    "    )\n",
    "\n",
    "    # Style displayed percentage numbers\n",
    "    for t in autotexts:\n",
    "        t.set_fontsize(10)\n",
    "        t.set_color(\"black\")\n",
    "\n",
    "    # Title\n",
    "    plt.title(f\"Social Group Distribution: {name}\", fontsize=16)\n",
    "\n",
    "    # Legend placed to the right of the figure\n",
    "    plt.legend(\n",
    "        wedges,\n",
    "        labels,\n",
    "        title=\"Domain\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        fontsize=11,\n",
    "        title_fontsize=12\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544d8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(texts: pd.Series) -> pd.Series:\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalise Russian stereotype strings.\n",
    "\n",
    "    Operations\n",
    "    ----------\n",
    "    - remove the phrases \", как и все люди,\" and \", как и все остальные,\"\n",
    "    - lowercase\n",
    "    - remove punctuation (including comma)\n",
    "    - replace '-' and '—' with spaces\n",
    "    - collapse multiple spaces\n",
    "    - normalise 'ё' → 'е'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : pd.Series\n",
    "        Series of raw text strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Normalised text strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove all punctuation except underscore (keep '_' if you use it as a token)\n",
    "    punc = ''.join(ch for ch in string.punctuation if ch not in '_')\n",
    "\n",
    "    # replace '-' and '—' with spaces, remove other punctuation (including commas)\n",
    "    trans_table = str.maketrans('-—', '  ', punc)\n",
    "\n",
    "    PHRASES_TO_REMOVE = [\n",
    "        \"как и все люди\",\n",
    "        \"как и все остальные\",\n",
    "        \"как и другие люди\",\n",
    "        \"как и другие народы\",\n",
    "        \"как и другие нации\" \n",
    "    ]\n",
    "\n",
    "    def _norm(s: str) -> str:\n",
    "        s = str(s)\n",
    "\n",
    "        # remove specific filler phrases\n",
    "        for ph in PHRASES_TO_REMOVE:\n",
    "            s = s.replace(ph, \"\")\n",
    "\n",
    "        # lowercase + strip punctuation\n",
    "        s = s.lower().translate(trans_table)\n",
    "\n",
    "        # collapse multiple spaces\n",
    "        s = \" \".join(s.split())\n",
    "\n",
    "        # normalise ё → е\n",
    "        s = s.replace(\"ё\", \"е\")\n",
    "\n",
    "        return s\n",
    "\n",
    "    return texts.apply(_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e282ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rubist(df: pd.DataFrame, text_col: str = \"text\") -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean and restructure the augmented RUBIAS dataset.\n",
    "\n",
    "    Steps:\n",
    "    - Drop rows where stereotype_type is missing\n",
    "    - Drop old 'category' column if present\n",
    "    - Rename 'label_level' → 'category'\n",
    "    - Create 'label' column: category_stereotype_type\n",
    "    - Reorder columns to: stereotype_type, text, category, label\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop rows with missing stereotype_type\n",
    "    df = df.dropna(subset=[\"stereotype_type\"])\n",
    "\n",
    "    # Drop old category column if exists\n",
    "    if \"category\" in df.columns:\n",
    "        df = df.drop(columns=[\"category\"])\n",
    "\n",
    "    # Rename label_level → category\n",
    "    df = df.rename(columns={\"label_level\": \"category\"})\n",
    "    \n",
    "    # Format strings\n",
    "    df[text_col] = format_text(df[text_col])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=\"text\")\n",
    "\n",
    "    # Reorder columns\n",
    "    desired_order = [\"stereotype_type\", text_col, \"category\"]\n",
    "    df = df[desired_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e2682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _grouped_barplot(percent_table, title, color_map):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot a grouped bar chart with thin bars and spacing.\n",
    "    Clean style, no y-label, no legend title.\n",
    "    \"\"\"\n",
    "\n",
    "    categories = percent_table.index.tolist()\n",
    "    labels = percent_table.columns.tolist()\n",
    "\n",
    "    x = np.arange(len(categories))\n",
    "\n",
    "    total_width = 0.55\n",
    "    width = total_width / max(len(labels), 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 3.5), dpi=200)\n",
    "\n",
    "    # Bars\n",
    "    for i, lab in enumerate(labels):\n",
    "        offsets = x - total_width/2 + (i + 0.5) * width\n",
    "        ax.bar(\n",
    "            offsets,\n",
    "            percent_table[lab].values,\n",
    "            width=width * 0.75,\n",
    "            label=lab,\n",
    "            color=color_map.get(lab, \"grey\"),\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "    # Axis Style\n",
    "    ax.set_facecolor(\"white\")\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, fontsize=11)\n",
    "\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks(np.arange(0, 101, 10))\n",
    "    ax.tick_params(axis='y', labelsize=9)\n",
    "    ax.set_ylabel(\"\")  # removed y-label\n",
    "\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, pos: f\"{int(v)}%\"))\n",
    "\n",
    "    ax.set_title(title, fontsize=15, pad=12)\n",
    "\n",
    "    ax.yaxis.grid(True, linestyle='-', alpha=0.13)\n",
    "\n",
    "    ax.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, -0.15),\n",
    "        ncol=len(labels),\n",
    "        frameon=False,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f403137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_by_category(df, category_col=\"category\", sentiment_col=\"sentiment\"):\n",
    "\n",
    "    tab = pd.crosstab(df[category_col], df[sentiment_col], normalize=\"index\") * 100\n",
    "\n",
    "    tab = tab.reindex(\n",
    "        index=[\"stereotype\", \"neutral\", \"unrelated\"],\n",
    "        columns=[\"positive\", \"neutral\", \"negative\"]\n",
    "    ).fillna(0)\n",
    "\n",
    "    _grouped_barplot(tab, \"Sentiment Classifications by Category (RuBIST)\", SENTIMENT_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691d3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toxic_by_category(df, category_col=\"category\", toxic_col=\"regard\"):\n",
    "\n",
    "    tab = pd.crosstab(df[category_col], df[toxic_col], normalize=\"index\") * 100\n",
    "\n",
    "    tab = tab.reindex(\n",
    "        index=[\"stereotype\", \"neutral\", \"unrelated\"],\n",
    "        columns=[\"toxic\", \"non_toxic\"]\n",
    "    ).fillna(0)\n",
    "\n",
    "    _grouped_barplot(tab, \"Toxicity Classifications by Category (RuBIST)\", TOXIC_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9d2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_for_stereotypes_by_group(df,\n",
    "                                            category_col=\"category\",\n",
    "                                            group_col=\"stereotype_type\",\n",
    "                                            sentiment_col=\"sentiment\"):\n",
    "\n",
    "    subset = df[df[category_col] == \"stereotype\"]\n",
    "\n",
    "    tab = pd.crosstab(subset[group_col], subset[sentiment_col], normalize=\"index\") * 100\n",
    "\n",
    "    tab = tab.reindex(\n",
    "        index=[\"gender\", \"profession\", \"nationality\", \"lgbtq\"],\n",
    "        columns=[\"positive\", \"neutral\", \"negative\"]\n",
    "    ).fillna(0)\n",
    "\n",
    "    _grouped_barplot(\n",
    "        tab,\n",
    "        \"Proportion of Sentiment Classifications\\nfor Stereotypical Sentences - By Group\",\n",
    "        SENTIMENT_COLORS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f2a55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toxic_for_stereotypes_by_group(df,\n",
    "                                        category_col=\"category\",\n",
    "                                        group_col=\"stereotype_type\",\n",
    "                                        toxic_col=\"regard\"):\n",
    "\n",
    "    subset = df[df[category_col] == \"stereotype\"]\n",
    "\n",
    "    tab = pd.crosstab(subset[group_col], subset[toxic_col], normalize=\"index\") * 100\n",
    "\n",
    "    tab = tab.reindex(\n",
    "        index=[\"gender\", \"profession\", \"nationality\", \"lgbtq\"],\n",
    "        columns=[\"toxic\", \"non_toxic\"]\n",
    "    ).fillna(0)\n",
    "\n",
    "    _grouped_barplot(\n",
    "        tab,\n",
    "        \"Proportion of Toxicity Classifications\\nfor Stereotypical Sentences - By Group\",\n",
    "        TOXIC_COLORS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f984125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_semantic_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"text\",\n",
    "    group_col: str = \"stereotype_type\",\n",
    "    model_name: str = \"DeepPavlov/rubert-base-cased-sentence\",\n",
    "    border_sim: float = 0.98,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove semantically near-duplicate text entries from a dataframe.\n",
    "\n",
    "    This function computes sentence embeddings using a SentenceTransformer\n",
    "    model and identifies near-duplicate sentences based on cosine similarity.\n",
    "    Only sentences belonging to the same group (e.g., same stereotype type)\n",
    "    are compared. For each pair of sentences that exceed the similarity \n",
    "    threshold, the later-indexed entry is removed. Detected duplicates \n",
    "    are printed to stdout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing at least the text column and optionally a \n",
    "        grouping column.\n",
    "    text_col : str, default \"text\"\n",
    "        Name of the column containing raw text to evaluate for duplicates.\n",
    "    group_col : str, default \"stereotype_type\"\n",
    "        Column name determining groups within which similarity comparisons \n",
    "        are performed. Sentences from different groups are never compared.\n",
    "    model_name : str, default \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "        Identifier of a SentenceTransformer model used to compute embeddings.\n",
    "    border_sim : float, default 0.98\n",
    "        Cosine similarity threshold above which two sentences are considered\n",
    "        near-duplicates. Must be in the range [0, 1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A cleaned dataframe with near-duplicate rows removed and the index\n",
    "        reset.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function prints each detected near-duplicate pair, including the\n",
    "      kept sentence, removed sentence, and similarity score.\n",
    "    - Duplicate detection is greedy: the earliest occurrence is preserved,\n",
    "      and any later duplicates are removed.\n",
    "    - Performance may degrade for very large datasets due to O(n^2)\n",
    "      pairwise similarity comparisons.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_clean = drop_semantic_duplicates(\n",
    "    ...     df,\n",
    "    ...     text_col=\"text\",\n",
    "    ...     group_col=\"stereotype_type\",\n",
    "    ...     border_sim=0.90,\n",
    "    ... )\n",
    "    >>> df_clean.head()\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=True).copy()\n",
    "\n",
    "    sent_encoder = SentenceTransformer(model_name)\n",
    "    texts = df[text_col].tolist()\n",
    "    embeddings = sent_encoder.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    to_remove = set()\n",
    "    n = len(df)\n",
    "\n",
    "    for i in range(n):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i + 1, n):\n",
    "            if j in to_remove:\n",
    "                continue\n",
    "\n",
    "            if df.loc[i, group_col] != df.loc[j, group_col]:\n",
    "                continue\n",
    "\n",
    "            sim = util.pytorch_cos_sim(embeddings[i], embeddings[j]).item()\n",
    "\n",
    "            if sim > border_sim:\n",
    "                print(\"-\" * 80)\n",
    "                print(f\"Duplicates Found (Similarity = {sim:.3f})\")\n",
    "                print(f\"Saved [{i}]: {df.loc[i, text_col]}\")\n",
    "                print(f\"Removed [{j}]: {df.loc[j, text_col]}\")\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "                to_remove.add(j)\n",
    "\n",
    "    print(f\"\\nTotal near-duplicates removed: {len(to_remove)}\\n\")\n",
    "\n",
    "    return df.drop(index=list(to_remove)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768697e1",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Justify architectural modifications for new context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddbf28",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aca6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "rubist_aug = pd.read_csv(\"COMP0173_Temp_Data/rubist_aug.csv\", encoding=\"utf-8\")\n",
    "rubist_aug_second= pd.read_csv(\"COMP0173_Temp_Data/rubist_aug_second.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a6fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "rubist_aug = clean_rubist(rubist_aug)\n",
    "rubist_aug.head()\n",
    "# Save cleaned final version\n",
    "rubist_aug.to_csv(\"COMP0173_Data/rubist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4045658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "rubist_aug_second = clean_rubist(rubist_aug_second)\n",
    "rubist_aug_second.head()\n",
    "# Save cleaned final version\n",
    "rubist_aug_second.to_csv(\"COMP0173_Data/rubist_second.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a8fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final version \n",
    "rubist = pd.read_csv(\"COMP0173_Data/rubist.csv\", encoding=\"utf-8\")\n",
    "rubist_second = pd.read_csv(\"COMP0173_Data/rubist_second.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8afc1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EDA\n",
    "target_dist = prepare_target_variable_distribution(rubist, \"category\")\n",
    "group_dist = prepare_group_distribution(rubist, \"stereotype_type\")\n",
    "text_length = prepare_text_length_analysis(rubist, \"text\")\n",
    "# create_word_cloud(rubist, text_col='text', output_filename='rubist_wordcloud.png')\n",
    "\n",
    "# Run EDA\n",
    "target_dist = prepare_target_variable_distribution(rubist_second, \"category\")\n",
    "group_dist = prepare_group_distribution(rubist_second, \"stereotype_type\")\n",
    "text_length = prepare_text_length_analysis(rubist_second, \"text\")\n",
    "# create_word_cloud(rubist_second, text_col='text', output_filename='rubist_second_wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f56762d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment and Toxicity\n",
    "# rubist_sentiment = analyse_sentiment_and_regard(rubist, text_col=\"text\")\n",
    "# rubist_sentiment.head()\n",
    "# rubist_sentiment.to_csv(\"COMP0173_Results/rubist_sentiment\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# # Sentiment and Toxicity\n",
    "# rubist_second_sentiment = analyse_sentiment_and_regard(rubist, text_col=\"text\")\n",
    "# rubist_second_sentiment.head()\n",
    "# rubist_second_sentiment.to_csv(\"COMP0173_Results/rubist_second_sentiment\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# rubist_sentiment = pd.read_csv(\"COMP0173_Results/rubist_sentiment\", encoding=\"utf-8-sig\")\n",
    "# rubist_second_sentiment = pd.read_csv(\"COMP0173_Results/rubist_second_sentiment\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "946531e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Colour maps\n",
    "# SENTIMENT_COLORS = {\n",
    "#     \"positive\": \"#559A67\",  \n",
    "#     \"neutral\":  \"#F1A72F\",  \n",
    "#     \"negative\": \"#CA5353\", \n",
    "# }\n",
    "\n",
    "# TOXIC_COLORS = {\n",
    "#     \"toxic\":      \"#CA5353\",\n",
    "#     \"non_toxic\":  \"#559A67\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26624ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate all four plots\n",
    "# plot_sentiment_by_category(rubist_sentiment)\n",
    "# plot_toxic_by_category(rubist_sentiment)\n",
    "# plot_sentiment_for_stereotypes_by_group(rubist_sentiment)\n",
    "# plot_toxic_for_stereotypes_by_group(rubist_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24807036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate all four plots\n",
    "# plot_sentiment_by_category(rubist_second_sentiment)\n",
    "# plot_toxic_by_category(rubist_second_sentiment)\n",
    "# plot_sentiment_for_stereotypes_by_group(rubist_second_sentiment)\n",
    "# plot_toxic_for_stereotypes_by_group(rubist_second_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d65e7",
   "metadata": {},
   "source": [
    "#### Train models - Logistic Regression (Spacy Russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad83c5bb-e6d8-448e-8b08-d00342a29652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from the training data:\n",
      "     stereotype_type                                               text  \\\n",
      "168       profession              футболисты топ клубов следят за модой   \n",
      "2883     nationality  британцы танцуют с радужными пельменями на кры...   \n",
      "2919           lgbtq  бисексуалы на самом деле просто сексуально рас...   \n",
      "3412      profession  программирование доступно людям с разным уровн...   \n",
      "732            lgbtq        пансексуалы просто до конца не определились   \n",
      "\n",
      "      category data_name  \n",
      "168          1    rubist  \n",
      "2883         0    rubist  \n",
      "2919         1    rubist  \n",
      "3412         0    rubist  \n",
      "732          1    rubist  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "2726      profession  бедные люди питаются звездным светом и варят с...   \n",
      "1066          gender  женщины водят машины распевая серенады с говор...   \n",
      "2547     nationality  евреи как и представители других национальност...   \n",
      "1834      profession  бедный человек должен танцевать с радужными но...   \n",
      "1825     nationality  евреи как и представители других национальност...   \n",
      "\n",
      "      category data_name  \n",
      "2726         0    rubist  \n",
      "1066         0    rubist  \n",
      "2547         0    rubist  \n",
      "1834         0    rubist  \n",
      "1825         0    rubist  \n",
      "Train data size: 3372\n",
      "Test data size: 844\n",
      "First few examples from the training data:\n",
      "     stereotype_type                                           text  category  \\\n",
      "1005          gender                     мужчинам нужен только секс         1   \n",
      "1001          gender           мужчина рисует на работу каждый день         0   \n",
      "2619     nationality               французы рисуют очень много сыра         0   \n",
      "2213          gender  женщина успешна независимо от наличия мужчины         0   \n",
      "934           gender    мужчина партнер семейства а женщина партнер         0   \n",
      "\n",
      "          data_name  \n",
      "1005  rubist_second  \n",
      "1001  rubist_second  \n",
      "2619  rubist_second  \n",
      "2213  rubist_second  \n",
      "934   rubist_second  \n",
      "First few examples from the testing data:\n",
      "     stereotype_type                                               text  \\\n",
      "80        profession             чтобы быть музыкантом нужно иметь шкаф   \n",
      "1814          gender                    все девочки пылесосят мальчиков   \n",
      "2277      profession  эйчар это человек который не взяли больше нику...   \n",
      "61        profession  чтобы стать музыкантом можно в любом возрасте ...   \n",
      "755           gender  мужчина представитель пушистого пола главная о...   \n",
      "\n",
      "      category      data_name  \n",
      "80           0  rubist_second  \n",
      "1814         0  rubist_second  \n",
      "2277         0  rubist_second  \n",
      "61           0  rubist_second  \n",
      "755          0  rubist_second  \n",
      "Train data size: 2336\n",
      "Test data size: 584\n",
      "Number of unique labels: 2\n",
      "Testing C=0.01, penalty=l1 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.9466666666666667\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.6326957285464712\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.6120484633882092\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.9466666666666667\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.8929148318366249\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.8824924844557498\n",
      "Testing C=1, penalty=None => F1 Score: 0.9466666666666667\n",
      "Best model parameters: {'C': 0.01, 'penalty': None}\n",
      "Model and vectorizer saved to model_output_LR_tfidf/rubist_trained\n",
      "Estimated total emissions: 7.616135099377501e-06 kg CO2\n",
      "Number of unique labels: 2\n",
      "Number of unique labels: 2\n",
      "Testing C=0.01, penalty=l1 => F1 Score: 0.4\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.4\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.5407413941385317\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.4\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.4\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.5407413941385317\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.46003885576472164\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.42689732142857145\n",
      "Testing C=1, penalty=None => F1 Score: 0.5407413941385317\n",
      "Best model parameters: {'C': 0.01, 'penalty': None}\n",
      "Model and vectorizer saved to model_output_LR_tfidf/rubist_second_trained\n",
      "Estimated total emissions: 6.865153047065397e-06 kg CO2\n",
      "Number of unique labels: 2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|████████████████| 3372/3372 [00:10<00:00, 316.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing C=0.01, penalty=l1 => F1 Score: 0.39785905441570024\n",
      "Testing C=0.01, penalty=l2 => F1 Score: 0.8377403846153846\n",
      "Testing C=0.01, penalty=None => F1 Score: 0.9196396682408032\n",
      "Testing C=0.1, penalty=l1 => F1 Score: 0.8833912566306933\n",
      "Testing C=0.1, penalty=l2 => F1 Score: 0.894211324570273\n",
      "Testing C=0.1, penalty=None => F1 Score: 0.9196396682408032\n",
      "Testing C=1, penalty=l1 => F1 Score: 0.9212241604072258\n",
      "Testing C=1, penalty=l2 => F1 Score: 0.9172978203631145\n",
      "Testing C=1, penalty=None => F1 Score: 0.9196396682408032\n",
      "Best model parameters: {'C': 1, 'penalty': 'l1'}\n",
      "Model and vectorizer saved to model_output_LR_embedding/rubist_trained\n",
      "Estimated total emissions: 7.494861642572302e-05 kg CO2\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████████████| 844/844 [00:02<00:00, 314.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|████████████████| 2336/2336 [00:06<00:00, 369.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing C=0.01, penalty=l1 => F1 Score: 0.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_rubist_second\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_base_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_output_LR_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrubist_second_trained\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m evaluate_model(test_data_rubist_second, model_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output_LR_embedding/rubist_second_trained\u001b[39m\u001b[38;5;124m'\u001b[39m, result_output_base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_output_LR_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m, dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrubist_second\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/tmp/HEARTS-Text-Stereotype-Detection/Model Training and Evaluation/Logistic_Regression_Russian.py:83\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_data, model_output_base_dir, dataset_name, feature_type, seed)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m penalty \u001b[38;5;129;01min\u001b[39;00m penalties:\n\u001b[1;32m     82\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(C\u001b[38;5;241m=\u001b[39mC, penalty\u001b[38;5;241m=\u001b[39mpenalty, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m---> 83\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     85\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/joblib/parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/joblib/parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:543\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    540\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[1;32m    541\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[0;32m--> 543\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[1;32m    564\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/linear_model/_sag.py:323\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    322\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[0;32m--> 323\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[1;32m    348\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    350\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    351\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Logistic_Regression_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "\n",
    "# Execute full pipeline for logistic regression tfidf model\n",
    "train_model(train_data_rubist, model_output_base_dir='model_output_LR_tfidf', dataset_name='rubist_trained', feature_type='tfidf', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_LR_tfidf/rubist_trained', result_output_base_dir='result_output_LR_tfidf', dataset_name='rubist', feature_type='tfidf', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_output_base_dir='model_output_LR_tfidf', dataset_name='rubist_second_trained', feature_type='tfidf', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_LR_tfidf/rubist_second_trained', result_output_base_dir='result_output_LR_tfidf', dataset_name='rubist_second', feature_type='tfidf', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Execute full pipeline for logistic regression embedding model\n",
    "train_model(train_data_rubist, model_output_base_dir='model_output_LR_embedding', dataset_name='rubist_trained', feature_type='embedding', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_LR_embedding/rubist_trained', result_output_base_dir='result_output_LR_embedding', dataset_name='rubist', feature_type='embedding', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_output_base_dir='model_output_LR_embedding', dataset_name='rubist_second_trained', feature_type='embedding', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_LR_embedding/rubist_second_trained', result_output_base_dir='result_output_LR_embedding', dataset_name='rubist_second', feature_type='embedding', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5379ef",
   "metadata": {},
   "source": [
    "#### Train models - DeepPavlov_rubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc88fe2-4054-4133-929e-9b1f9386c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/hf\"\n",
    "os.makedirs(\"/tmp/hf\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='DeepPavlov/rubert-base-cased', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_deeppavlov_rubert', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_deeppavlov_rubert/rubist_trained', result_output_base_dir='result_output_deeppavlov_rubert', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='DeepPavlov/rubert-base-cased', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_deeppavlov_rubert', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_deeppavlov_rubert/rubist_second_trained', result_output_base_dir='result_output_deeppavlov_rubert', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344099e",
   "metadata": {},
   "source": [
    "#### Train models - roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c17623-ba4a-4939-b4cb-a446bf89c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='ai-forever/ruBert-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_ruberta_base', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_ruberta_base/rubist_trained', result_output_base_dir='result_output_ruberta_base', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='ai-forever/ruBert-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_ruberta_base', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_ruberta_base/rubist_second_trained', result_output_base_dir='result_output_ruberta_base', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a843b28",
   "metadata": {},
   "source": [
    "#### Train models - FacebookAI/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42933b8c-346b-4230-b6a2-b66c317589f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_Models_Fine_Tuning_Russian import (data_loader, train_model, evaluate_model)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load and combine relevant datasets\n",
    "train_data_rubist, test_data_rubist = data_loader(csv_file_path='COMP0173_Data/rubist.csv', labelling_criteria='stereotype', dataset_name='rubist', sample_size=1000000, num_examples=5)\n",
    "train_data_rubist_second, test_data_rubist_second = data_loader(csv_file_path='COMP0173_Data/rubist_second.csv', labelling_criteria='stereotype', dataset_name='rubist_second', sample_size=1000000, num_examples=5)\n",
    "\n",
    "# Execute full pipeline for Deepavlov model\n",
    "train_model(train_data_rubist, model_path='FacebookAI/xlm-roberta-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_xlm_roberta_base', dataset_name='rubist_trained', seed=42)\n",
    "evaluate_model(test_data_rubist, model_output_dir='model_output_xlm_roberta_base/rubist_trained', result_output_base_dir='result_output_xlm_roberta_base', dataset_name='rubist_trained', seed=42)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_model(train_data_rubist_second, model_path='FacebookAI/xlm-roberta-base', batch_size=64, epoch=6, learning_rate=2e-5, model_output_base_dir='model_output_xlm_roberta_base', dataset_name='rubist_second_trained', seed=42)\n",
    "evaluate_model(test_data_rubist_second, model_output_dir='model_output_xlm_roberta_base/rubist_second_trained', result_output_base_dir='result_output_xlm_roberta_base', dataset_name='rubist_second_trained', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40842ea4",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Document hyperparameter tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d59de",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate the adapted model, comparing performance metrics with the original study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be6f894",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Compare original vs. adapted model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0eb46",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Use appropriate metrics for problem type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc9723",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 3:}$ Conduct statistical significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a40030",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 4:}$ Analyze failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad2c76",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[1] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS: A holistic framework for explainable, sustainable and robust text stereotype detection.\n",
    "arXiv preprint arXiv:2409.11579.\n",
    "Available at: https://arxiv.org/abs/2409.11579\n",
    "(Accessed: 4 December 2025).\n",
    "https://doi.org/10.48550/arXiv.2409.11579\n",
    "\n",
    "[2] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS-Text-Stereotype-Detection (GitHub Repository).\n",
    "Available at: https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[3] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. Holistic AI. 2024.\n",
    "EMGSD: Expanded Multi-Group Stereotype Dataset (HuggingFace Dataset).\n",
    "Available at: https://huggingface.co/datasets/holistic-ai/EMGSD\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[4] University College London Technical Support Group (TSG).\n",
    "2025. GPU Access and Usage Documentation.\n",
    "Available at: https://tsg.cs.ucl.ac.uk/gpus/\n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[5] United Nations. 2025. The 2030 Agenda for Sustainable Development. \n",
    "Available at: https://sdgs.un.org/2030agenda \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8f621",
   "metadata": {},
   "source": [
    "[] Natasha. Russian NLP Library (conda-forge distribution). Available at:\n",
    "https://anaconda.org/conda-forge/natasha\n",
    "(Accessed 8 December 2025).\n",
    "\n",
    "[] Anthropic. Claude Artifact. Available at:\n",
    "https://claude.ai/public/artifacts/ab5532d8-7d61-4a98-acec-5cc4236f0d74\n",
    "(Accessed: 8 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c5643",
   "metadata": {},
   "source": [
    "## References: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7ae45",
   "metadata": {},
   "source": [
    "Gender Stereotypes\n",
    "\n",
    "https://adme.media/articles/11-stereotipov-o-muzhchinah-i-zhenschinah-kotorye-davno-ustareli-no-mnogie-s-nimi-tak-i-zhivut-2526726/\n",
    "\n",
    "https://t-j.ru/gender-stereotypes-cases/?utm_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "https://klinikaexpert.ru/articles/v-yarlykah-lozhnye-stereotipy-o-muzhchinah-i-zhenschinah\n",
    "\n",
    "https://news.zerkalo.io/life/54154.html\n",
    "\n",
    "https://www.sbras.info/articles/editors/gendernye-stereotipy-v-kotorye-pora-perestat-verit\n",
    "\n",
    "https://www.rbc.ua/ukr/styler/rozpovsyudzheni-ta-zastarili-stereotipi-cholovikiv-1709482914.html\n",
    "\n",
    "https://burninghut.ru/stereotipy-o-muzhchinakh-i-zhenshhinakh/\n",
    "\n",
    "https://www.sravni.ru/text/5-stereotipov-o-muzhchinakh-i-zhenshhinakh-kotorye-plokho-vlijajut-na-finansy-semi/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb6af1",
   "metadata": {},
   "source": [
    "Profession Stereotypes\n",
    "\n",
    "https://psy.1sept.ru/article.php?ID=200301712\n",
    "\n",
    "https://peopletalk.ru/article/10-samyh-populyarnyh-stereotipov-o-professiyah-kotorye-besyat/\n",
    "\n",
    "https://adukar.com/by/news/abiturientu/stereotipy-o-professiyah\n",
    "\n",
    "https://kemgmu.ru/about_the_university/news/11672/\n",
    "\n",
    "https://dzen.ru/a/YZ81OX7HcALiVr1k\n",
    "\n",
    "https://nafi.ru/en/analytics/samye-rasprostranennye-stereotipy-rossiyan-ob-it-professiyakh-/\n",
    "\n",
    "https://mosdigitals.ru/blog/pochemu-ne-lyubyat-yuristov-osnovnye-prichiny-i-stereotipy\n",
    "\n",
    "https://medium.com/juris-prudence/%D1%82%D1%80%D0%B0%D0%B4%D0%B8%D1%86%D0%B8%D0%B8-%D1%8E%D1%80%D0%B8%D1%81%D1%82%D0%BE%D0%B2-%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%B0-%D0%B2%D1%8B%D0%BC%D1%8B%D1%81%D0%B5%D0%BB-%D0%B1%D1%83%D0%BB%D1%88%D0%B8%D1%82-cc3de98cdfec\n",
    "\n",
    "https://stereotypes_actors.tilda.ws/\n",
    "\n",
    "https://m.vk.com/wall-181816199_1644\n",
    "\n",
    "https://kovrov.ecvdo.ru/states/stereotipy-o-professii-ekonomista-chto-pravda-a-chto-vymysel\n",
    "\n",
    "https://urzhum.ecvdo.ru/states/razvenchivaem-mify-o-professii-finansista\n",
    "\n",
    "https://azov.ecvdo.ru/states/mify-o-populyarnyh-professiyah?utm_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "https://igrim.ecvdo.ru/states/mify-i-pravda-o-rabote-v-sfere-obrazovaniya?utm_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "https://brodude.ru/8-stereotipov-o-rabote-shef-povara/\n",
    "\n",
    "https://www.maximonline.ru/lifestyle/7-glavnykh-mifov-o-rabote-bortprovodnikov-id6443401/\n",
    "\n",
    "https://www.sports.ru/football/blogs/477244.html\n",
    "\n",
    "https://dnmu.edu.ua/old/pro-meditsinu/3633-o-hirurgah\n",
    "\n",
    "https://vc.ru/id3158218/1127046-mify-o-pilotah-ty-tozhe-tak-dumal\n",
    "\n",
    "https://www.psychologies.ru/wellbeing/5-strashnyih-mifov-o-detskom-balete-v-kotoryie-pora-perestat-verit/\n",
    "\n",
    "https://masterok.livejournal.com/11781279.html\n",
    "\n",
    "https://adme.media/articles/10-mifov-o-balete-kotorye-kinoshniki-pridumali-radi-vau-effekta-a-my-i-kupilis-2514646/\n",
    "\n",
    "https://mir24.tv/articles/16379683/8-glavnyh-stereotipov-o-rabote-advokata-merkantilnye-ciniki-ili-professionaly\n",
    "\n",
    "https://omsk.ecvdo.ru/states/mify-o-dizajnerskoj-professii-razbiraemsya-chto-pravda-a-chto-net?utm_referrer=https%3A%2F%2Fwww.google.com%2F\n",
    "\n",
    "https://media.contented.ru/vdohnovenie/kofebrejk/5-mifov-o-dizaynerah/\n",
    "\n",
    "https://ashleyhome.am/ru/blogs/news/steriotipy-o-dizainerax\n",
    "\n",
    "https://lifehacker.ru/7-stereotipov-o-rabote-barmena/\n",
    "\n",
    "https://klepachsv.livejournal.com/24127.html\n",
    "\n",
    "https://www.championat.com/lifestyle/article-4793635-razoblachaem-top-9-stereotipov-o-sportsmenah-pravda-ili-vymysel.html\n",
    "\n",
    "https://krasotuli.com/25199-stereotipy-o-parikmaherskom-dele-razvenchivaem-mify.html\n",
    "\n",
    "https://maycenter.ru/blog/mifi-o-professii-parikmakhera\n",
    "\n",
    "https://m.ok.ru/group/70000000389722/topic/157405964165210?opncmnt\n",
    "https://nacasting.ru/statii/mify-o-rezhisserakh\n",
    "\n",
    "https://omsk.ecvdo.ru/states/mify-o-professii-arhitektora-chto-na-samom-dele-vazhno-dlya-uspeha-v-etoj-sfere\n",
    "\n",
    "http://www.lookatme.ru/flow/posts/fashion-radar/181029-mify-i-realii-o-modelnom-biznese\n",
    "\n",
    "https://rylskova.com/%D0%BC%D0%B8%D1%84%D1%8B-%D0%BE-%D0%BF%D1%80%D0%BE%D1%84%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85-%D1%84%D0%BE%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B0%D1%85/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec28403",
   "metadata": {},
   "source": [
    "Nationality/Race Stereotypes\n",
    "\n",
    "https://kuban24.tv/item/10-samyh-rasprostranennyh-stereotipov-o-raznyh-natsionalnostyah\n",
    "\n",
    "https://mir24.tv/articles/16626782/10-stereotipov-ob-indejcah:-razoblachenie-mifov-i-udivitelnye-fakty\n",
    "\n",
    "https://tandem.net/ru/blog/russian-stereotypes-fact-fiction\n",
    "\n",
    "https://tandem.net/ru/blog/british-stereotypes-fact-or-myth\n",
    "\n",
    "https://meschool.ru/poleznoe/top-7-stereotipov-o-britancakh/\n",
    "\n",
    "https://linguacats.com/ru/stati/o-chjom-molchat-frantsuzhenki\n",
    "\n",
    "https://smapse.livejournal.com/699411.html\n",
    "\n",
    "https://francaisclub.ru/%D1%81%D1%82%D0%B5%D1%80%D0%B5%D0%BE%D1%82%D0%B8%D0%BF%D1%8B-%D0%BE-%D1%84%D1%80%D0%B0%D0%BD%D1%86%D1%83%D0%B7%D0%B0%D1%85/\n",
    "\n",
    "https://www.bestprivateguides.com/articles/stereotipi-ob-italyantsah-art-69.php\n",
    "\n",
    "https://maminklub.lv/rebionok/stereotipy-ob-ispantsakh-pravda-i-lozh-623074/\n",
    "\n",
    "https://pikabu.ru/story/stereotipyi_ob_ispantsakh__pravda_i_vyimyisel_6306872\n",
    "\n",
    "https://chetyre-jelania.livejournal.com/197804.html\n",
    "\n",
    "https://www.staypoland.com/ru/poland/stereotipy-o-polshe/\n",
    "\n",
    "https://abea.com.ua/ru/top-10-pravdyvykh-stereotypov-ob-ukrayne-y-ukrayntsakh\n",
    "\n",
    "https://vancouverok.com/15-stereotipov-o-kanadtsah-kotorye-yavlyayutsya-pravdoj/\n",
    "\n",
    "https://nashvancouver.com/6-lozhnih-stereotipov-o-kanadcax/\n",
    "\n",
    "https://amivisa.ru/blog/usa/stereotipy-ob-amerikancax-pravda-i-vymysel/\n",
    "\n",
    "https://www.english-language.ru/articles/informative/stereotipyi-ob-amerikanczax/\n",
    "\n",
    "https://tonkosti.ru/%D0%96%D1%83%D1%80%D0%BD%D0%B0%D0%BB/11_%D0%B4%D0%B8%D0%BA%D0%BE%D0%B2%D0%B0%D1%82%D1%8B%D1%85_%D1%81%D1%82%D0%B5%D1%80%D0%B5%D0%BE%D1%82%D0%B8%D0%BF%D0%BE%D0%B2_%D0%BE_%D1%80%D1%83%D1%81%D1%81%D0%BA%D0%B8%D1%85,_%D0%B2_%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B5_%D0%B2%D0%B5%D1%80%D1%8F%D1%82_%D0%B0%D0%BC%D0%B5%D1%80%D0%B8%D0%BA%D0%B0%D0%BD%D1%86%D1%8B\n",
    "\n",
    "https://dzen.ru/a/ZfHtqQPJj3LVcVWV\n",
    "\n",
    "https://www.reddit.com/r/AskCentralAsia/comments/ahes8h/what_are_the_stereotypes_of_the_different_central/?tl=ru\n",
    "\n",
    "https://am.tsargrad.tv/articles/5-stereotipov-pro-armjan-i-armeniju_395939\n",
    "\n",
    "https://adme.media/articles/ya-zhivu-v-irane-i-hochu-rasskazat-o-10-veschah-kotorye-otkroyut-etu-stranu-s-drugoj-storony-1859715/\n",
    "\n",
    "https://chilltravel.ru/iindiastereotipi\n",
    "\n",
    "https://www.chaochay.ru/blog/9-mifov-o-kitae-i-kitajcah\n",
    "\n",
    "https://smapse.ru/7-banalnyh-stereotipov-o-zhitelyah-yuzhnoj-korei/\n",
    "\n",
    "https://lifehacker.ru/stereotipy-o-severnoi-koree/\n",
    "\n",
    "https://moya-planeta.ru/reports/view/yaponcy_lomka_stereotipov_35074\n",
    "\n",
    "https://smapse.ru/15-stereotipov-o-yaponcah-kotorye-oni-nenavidyat/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70db2df",
   "metadata": {},
   "source": [
    "LGBTQ+ Stereotypes\n",
    "\n",
    "https://denis-balin.livejournal.com/329915.html\n",
    "\n",
    "https://sojka.io/ru/guides/lgbt\n",
    "\n",
    "http://raznoobrasije.org/wp-content/uploads/2020/07/2020_Raznoobrasije_1_Mythen-und-Fakten-u%CC%88ber-LGB.pdf\n",
    "\n",
    "https://spherequeer.org/bisexual-week-2023/\n",
    "\n",
    "https://gpress.info/2020/03/13/stereotipy-o-lgbt-1/\n",
    "\n",
    "https://parniplus.com/lgbt-movement/myths-about-bisexuality/\n",
    "\n",
    "\n",
    "https://www.kok.team/ru/2018-04-26/stereotipy-o-lesbiyankah\n",
    "\n",
    "https://yvision.kz/post/gei-i-lesbiyanki-mify-i-fakty-seksualnaya-patologiya-ili-estestvennyy-process-298823\n",
    "\n",
    "https://whatisgood.ru/theory/analytics/ulovki-lgbt-propagand/\n",
    "\n",
    "https://holod.media/2023/05/15/myths-about-trans-people/\n",
    "\n",
    "https://vk.com/@ovsyanart-trans-people\n",
    "\n",
    "https://rostovgazeta.ru/news/2017-02-17/samye-rasprostranennye-mify-o-transgenderah-1353439?utm_source=google.com&utm_medium=organic&utm_campaign=google.com&utm_referrer=google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59605bd9",
   "metadata": {},
   "source": [
    "## References  - Stereotype\n",
    "\n",
    "[24] Kaustubh Shivshankar Shejole and Pushpak Bhattacharyya. 2025.  \n",
    "StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way  \n",
    "Using Social Psychological Underpinnings. arXiv preprint arXiv:2504.03352.  \n",
    "Available at: https://arxiv.org/abs/2504.03352  \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da557ffe",
   "metadata": {},
   "source": [
    "## References: RuHateBe\n",
    "\n",
    "[6] Anna Palatkina, Elisey Rykov, Elina Sigdel, and Anna Sukhanova. 2024. \n",
    "RUHABE: Russian Hate Speech Benchmark. \n",
    "Available at: https://disk.360.yandex.ru/i/Divcpu7LaJwchw  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[7] Anna Palatkina, Elisey Rykov, Elina Sigdel, and Anna Sukhanova. 2024. \n",
    "RUHABE Dataset. \n",
    "Available at: https://disk.360.yandex.ru/d/hi3PF0XuoyCRlg  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[8] Anna Palatkina, Elisey Rykov, Elina Sigdel, and Anna Sukhanova. 2024. \n",
    "RUHABE Website (GitHub Repository). \n",
    "Available at: https://github.com/Annasuhstuff/RUHABE-website \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f0706",
   "metadata": {},
   "source": [
    "## References: Russian Distorted Toxicity\n",
    "\n",
    "[12] Alla Goryacheva. 2023. Toxicity Detection in Russian: Thesis Project Repository.  \n",
    "GitHub Repository. Available at: https://github.com/alla-g/toxicity-detection-thesis/  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[13] Alla Goryacheva. 2023. Russian Distorted Toxicity Corpus (TSV file).  \n",
    "In *Toxicity Detection in Russian: Thesis Project Repository*.  \n",
    "Available at: https://github.com/alla-g/toxicity-detection-thesis/blob/main/toxicity_corpus/russian_distorted_toxicity.tsv  \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8990f5",
   "metadata": {},
   "source": [
    "## References: Kaggle - Russian Language Toxic Comments\n",
    "\n",
    "[14] Blackmoon. 2019. Russian Language Toxic Comments Dataset.  \n",
    "Kaggle. Available at: https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[15] Sergey Smetanin. 2020. Toxic Comments Detection in Russian.  \n",
    "In *Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference “Dialogue 2020”*.  \n",
    "Available at: https://doi.org/10.28995/2075-7182-2020-19-1149-1159  \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425235a9",
   "metadata": {},
   "source": [
    "## References: Kaggle - Russian Hate Speech Recognition\n",
    "\n",
    "[23] Kamil Saitov and Leon Derczynski. 2021.  \n",
    "Abusive Language Recognition in Russian.  \n",
    "In *Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing*,  \n",
    "Kiyv, Ukraine, 20–25. Association for Computational Linguistics.  \n",
    "Available at: https://aclanthology.org/2021.bsnlp-1.3/  \n",
    "(Accessed: 7 December 2025).\n",
    "\n",
    "[20] Kamil Saitov and Leon Derczynski. 2021.   \n",
    "Russian Hate Speech Recognition (GitHub Repository).  \n",
    "Available at: https://github.com/Sariellee/Russan-Hate-speech-Recognition \n",
    "(Accessed: 6 December 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9678ac",
   "metadata": {},
   "source": [
    "## References: Kaggle - Misc\n",
    "\n",
    "[16] Bertie Vidgen and Leon Derczynski. 2020.  \n",
    "Directions in abusive language training data, a systematic review: Garbage in, garbage out.  \n",
    "*PLOS ONE*, 15, 12, e0243300.  \n",
    "Available at: https://doi.org/10.1371/journal.pone.0243300  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[17] Fabio Poletto, Valerio Basile, Manuela Sanguinetti, Cristina Bosco, and Viviana Patti. 2021.  \n",
    "Resources and benchmark corpora for hate speech detection: A systematic review.  \n",
    "*Language Resources & Evaluation*, 55, 477–523.  \n",
    "Available at: https://doi.org/10.1007/s10579-020-09502-8  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[18] Surendrabikram Thapa, Aditya Shah, Farhan Jafri, Usman Naseem, and Imran Razzak. 2022.  \n",
    "A Multi-Modal Dataset for Hate Speech Detection on Social Media: Case-study of Russia–Ukraine Conflict.  \n",
    "In *Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)*,  \n",
    "1–6. Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.  \n",
    "Available at: https://aclanthology.org/2022.case-1.1  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[19] Surendrabikram Thapa, Farhan Ahmad Jafri, Kritesh Rauniyar, Mehwish Nasim, and Usman Naseem. 2024.  \n",
    "RUHate-MM: Identification of Hate Speech and Targets using Multimodal Data from Russia–Ukraine Crisis.  \n",
    "In *Companion Proceedings of the ACM Web Conference 2024 (WWW '24)*.  \n",
    "Association for Computing Machinery, New York, NY, USA, 1854–1863.  \n",
    "Available at: https://doi.org/10.1145/3589335.3651973  \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[21] Ekaterina Pronoza, Polina Panicheva, Olessia Koltsova, and Paolo Rosso. 2021.  \n",
    "Detecting ethnicity-targeted hate speech in Russian social media texts.  \n",
    "Information Processing & Management, 58, 6 (2021), 102674.  \n",
    "Available at: https://www.sciencedirect.com/science/article/pii/S0306457321001606  \n",
    "(Accessed: 6 December 2025).  \n",
    "https://doi.org/10.1016/j.ipm.2021.102674\n",
    "\n",
    "[22] X. Wen, Y. Wang, K. Wang, and R. Sui. 2022.  \n",
    "A Russian Hate Speech Corpus for Cybersecurity Applications.  \n",
    "In *Proceedings of the 2022 IEEE 8th International Conference on Big Data Security on Cloud (BigDataSecurity),  \n",
    "IEEE International Conference on High Performance and Smart Computing (HPSC) and  \n",
    "IEEE International Conference on Intelligent Data and Security (IDS)*, Jinan, China, 41–47.  \n",
    "Available at: https://doi.org/10.1109/BigDataSecurityHPSCIDS54978.2022.00018  \n",
    "(Accessed: 6 December 2025)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (GPU)",
   "language": "python",
   "name": "hearts_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
