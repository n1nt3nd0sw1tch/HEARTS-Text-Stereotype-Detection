{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c925ac8a",
   "metadata": {},
   "source": [
    "# COMP0173: Coursework 2\n",
    "\n",
    "The paper HEARTS: A Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection by Theo King, Zekun Wu et al. (2024) presents a comprehensive approach to analysing and detecting stereotypes in text [1]. The authors introduce the HEARTS framework, which integrates model explainability, carbon-efficient training, and accurate evaluation across multiple bias-sensitive datasets. By using transformer-based models such as ALBERT-V2, BERT, and DistilBERT, this research project demonstrates that stereotype detection performance varies significantly across dataset sources, underlining the need for diverse evaluation benchmarks. The paper provides publicly available datasets and code [2], allowing full reproducibility and offering a standardised methodology for future research on bias and stereotype detection in Natural Language Processing (NLP).\n",
    "\n",
    "While the HEARTS framework evaluates stereotype detection in English, this project adapts the methodology to the Russian context. Russian stereotypes often rely on grammatical gender, morphology, and culture specific tropes. Although Russian is not classified as a low-resource language and many high-performing NLP models are available, there is currently no publicly accessible model specifically designed to detect stereotypes in Russian language. Existing models detecting toxicity or sentiment identify stereotypical and biased sentences only when they include specific patterns, such as insults, slurs, or identity-specific hate speech [8]. \n",
    "\n",
    "To address this gap, I introduce two fine-tuned classifiers, `AI-Forever-RuBert` [10] and `XML-RoBERTa` [11] trained on datasets `RBSA`, and `RBS`, respectively. Understanding these patterns is essential for applications such as content moderation, ensuring the safety of Russian-language LLMs, and monitoring harmful narratives across demographic groups and underrepresented societies. Adapting the HEARTS framework to this new sociolinguistic context illustrates its transferability beyond the English-speaking context and enables a more culturally grounded approach to bias detection, thereby promoting SDG 5: Gender Equality, SDG 10: Reduced Inequalities, and SDG 16: Peace, Justice, and Strong Institutions [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068fd33",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "All figures produced during this notebook are stored in the project’s `COMP0173_Figures` directory.\n",
    "The corresponding LaTeX-formatted performance comparison tables, jupyter notebooks are stored in `/COMP0173_PDF`. \n",
    "The compiled document are available as `COMP0173-CW2-TABLES.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-XX.pdf`.\n",
    "All prompts used for data augmentation are stored in `COMP0173_Prompts` and the manually collected stereotypes (with English translations) are provided in `COMP0173_Stereotypes`. \n",
    "The datasets used for model training and evaluation are stored in `COMP0173_Data` which contains: \n",
    "\n",
    "- rubias.tsv — RuBias dataset [6, 7]\n",
    "- ruster.csv — RuSter dataset (see Part 2 of the notebook for source websites)\n",
    "- rubist.csv — RBS dataset: RuBias + RuSter augmented with LLM-generated samples (Claude Sonnet), using a zero-shot prompt with examples\n",
    "- rubist_second.csv — RBSA dataset: RuBias + RuSter augmented with LLM-generated samples using a second prompt version without examples\n",
    "\n",
    "The notebooks `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P3.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P5.pdf` are replications of `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P2.pdf` and `COMP0173_PDF/COMP0173-CW2-NOTEBOOK-P4.pdf`, where P2 provides the new `RBSA` with second prompt (without examples) and P5 demonstrates the model running ON GPU (the results saved are from GPU fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279720f4",
   "metadata": {},
   "source": [
    "# Technical Implementation (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526aa252-67bc-4800-a639-661ef2b90b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import random, numpy as np, torch\n",
    "import pandas as pd\n",
    "\n",
    "import platform\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import spacy \n",
    "import os\n",
    "import sys\n",
    "import importlib.util, pathlib\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "from importlib import reload\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de35ebc-a91d-4e60-be6c-b303ef01d292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU host (UCL access)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7832d-2dcc-4052-a13c-8b7bac00f325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/HEARTS-Text-Stereotype-Detection'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path\n",
    "import os\n",
    "os.chdir(\"/tmp/HEARTS-Text-Stereotype-Detection\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ddc52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part 1: Replicate the baseline AI methodology using the open dataset \n",
    "\n",
    "The HEARTS framework evaluates stereotype detection using four publicly available text datasets [3]. The following datasets are Multi-Grain Stereotype Dataset (MGSD), Augmented WinoQueer (AWinoQueer), Augmented SeeGULL (ASeeGULL), and Expanded Multi-Grain Stereotype Dataset (EMGSD), which includes labelled stereotypical and non-stereotypical statements covering gender, profession, nationality, race, religion, and LGBTQ+ stereotypes. All datasets referenced in the paper are openly accessible through the associated GitHub repository [2] and Kaggle [3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3e09e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c99553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_summary(project_root: Path) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Collect summary metrics from all model output folders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_root : Path\n",
    "        Path to the root directory containing result_output_* folders.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A table containing model type, training dataset, evaluation dataset,\n",
    "        accuracy, macro precision, macro recall, and macro F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    # Loop through each model type directory\n",
    "    for model_dir in [\n",
    "        \"result_output_albertv2\",\n",
    "        \"result_output_bert\",\n",
    "        \"result_output_distilbert\",\n",
    "    ]:\n",
    "        base = project_root / model_dir\n",
    "        if not base.exists():\n",
    "            continue\n",
    "\n",
    "        # Standardise model name\n",
    "        model_name = model_dir.replace(\"result_output_\", \"\")\n",
    "\n",
    "        # Iterate through training subsets\n",
    "        for train_dir in sorted(base.iterdir()):\n",
    "            if not train_dir.is_dir():\n",
    "                continue\n",
    "            train_name = train_dir.name\n",
    "\n",
    "            # Iterate through evaluation subsets\n",
    "            for eval_dir in sorted(train_dir.iterdir()):\n",
    "                if not eval_dir.is_dir():\n",
    "                    continue\n",
    "                eval_name = eval_dir.name\n",
    "\n",
    "                report_path = eval_dir / \"classification_report.csv\"\n",
    "                if not report_path.exists():\n",
    "                    continue\n",
    "\n",
    "                # Load sklearn classification_report CSV\n",
    "                rep = pd.read_csv(report_path, index_col=0)\n",
    "\n",
    "                # Extract accuracy and macro-level metrics\n",
    "                acc = rep.loc[\"accuracy\", \"precision\"]  # sklearn stores accuracy here\n",
    "                macro_f1 = rep.loc[\"macro avg\", \"f1-score\"]\n",
    "                macro_prec = rep.loc[\"macro avg\", \"precision\"]\n",
    "                macro_rec = rep.loc[\"macro avg\", \"recall\"]\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"model\": model_name,\n",
    "                        \"train_on\": train_name,\n",
    "                        \"eval_on\": eval_name,\n",
    "                        \"accuracy\": acc,\n",
    "                        \"macro_precision\": macro_prec,\n",
    "                        \"macro_recall\": macro_rec,\n",
    "                        \"macro_f1\": macro_f1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Construct a pivot table matching the layout of the HEARTS paper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_df : pd.DataFrame\n",
    "        DataFrame containing model, training set, evaluation set, and metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Pivot table indexed by (model, training dataset short name) with\n",
    "        evaluation datasets as columns and macro-F1 scores as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add short labels using mapping dictionaries\n",
    "    df = summary_df.assign(\n",
    "        train_short=lambda d: d[\"train_on\"].map(train_map),\n",
    "        eval_short=lambda d: d[\"eval_on\"].map(eval_map),\n",
    "    )\n",
    "\n",
    "    # Remove rows where mapping failed\n",
    "    df = df.dropna(subset=[\"train_short\", \"eval_short\"])\n",
    "\n",
    "    # Create pivot table: rows = model x train_set, cols = eval_set\n",
    "    table = df.pivot_table(\n",
    "        index=[\"model\", \"train_short\"], columns=\"eval_short\", values=\"macro_f1\"\n",
    "    ).sort_index()\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798713bb",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Clone the original repository successfully \n",
    "\n",
    "The code for HEARTS was obtained by cloning a fork of the official repository [2]. The upstream repository, authored by Holistic AI, was then linked to provide access to the original implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7335402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone fork\n",
    "# git clone https://github.com/n1nt3nd0sw1tch/HEARTS-Text-Stereotype-Detection.git\n",
    "# cd HEARTS-Text-Stereotype-Detection\n",
    "\n",
    "# # Link to the original repo\n",
    "# git remote add upstream https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection.git\n",
    "# git remote -v   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf01c4",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Document all dependencies and environment setup\n",
    "\n",
    "A virtual environment was installed to isolate the HEARTS experimentation setup and ensure reproducibility. In the GPU-powered environment, the repository and data paths, along with system and library versions, were set up and confirmed [4]. All four datasets required by the HEARTS pipeline were confirmed to be present in the configured data directory.\n",
    "\n",
    "**Runtime configuration on the UCL GPU node:**\n",
    "\n",
    "- Working directory: /tmp/HEARTS-Text-Stereotype-Detection\n",
    "\n",
    "- Python: 3.9.21\n",
    "\n",
    "- Platform: Linux (x86_64, glibc 2.34)\n",
    "\n",
    "- PyTorch: 2.8.0+cu128\n",
    "\n",
    "- CUDA: available (NVIDIA GeForce RTX 3090 Ti) / (NVIDIA Tesla 4)\n",
    "\n",
    "- Transformers: 4.57.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and activate conda environment\n",
    "# conda create -n hearts python=3.10 -y\n",
    "# conda activate hearts\n",
    "\n",
    "# # Register environment as a Jupyter kernel\n",
    "# python -m pip install ipykernel\n",
    "# python -m ipykernel install --user\n",
    "\n",
    "# # Install repository dependencies\n",
    "# pip install -r requirements.txt\n",
    "# pip install --no-cache-dir -r requirements.txt\n",
    "# pip install --no-cache-dir spacy\n",
    "# pip install --no-cache-dir scikit-learn\n",
    "# pip install codecarbon\n",
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56086efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerate==1.1.1\n",
    "# aiohappyeyeballs==2.4.3\n",
    "# aiohttp==3.11.8\n",
    "# aiosignal==1.3.1\n",
    "# annotated-types==0.7.0\n",
    "# anyio==4.6.2.post1\n",
    "# appnope==0.1.4\n",
    "# arrow==1.3.0\n",
    "# asttokens==3.0.1\n",
    "# async-timeout==5.0.1\n",
    "# attrs==24.2.0\n",
    "# beautifulsoup4==4.14.3\n",
    "# blis==1.0.1\n",
    "# Bottleneck==1.4.2\n",
    "# catalogue==2.0.10\n",
    "# certifi==2024.8.30\n",
    "# cffi==1.17.1\n",
    "# charset-normalizer==3.4.0\n",
    "# click==8.1.7\n",
    "# cloudpathlib==0.16.0\n",
    "# cloudpickle==3.1.0\n",
    "# codecarbon==2.8.0\n",
    "# colorama==0.4.6\n",
    "# comm==0.2.3\n",
    "# confection==0.1.5\n",
    "# contourpy==1.3.1\n",
    "# cryptography==44.0.0\n",
    "# cycler==0.12.1\n",
    "# cymem==2.0.10\n",
    "# datasets==3.1.0\n",
    "# DAWG-Python==0.7.2\n",
    "# DAWG2-Python==0.9.0\n",
    "# debugpy==1.8.16\n",
    "# decorator==5.2.1\n",
    "# deep-translator==1.11.4\n",
    "# dill==0.3.8\n",
    "# distro==1.9.0\n",
    "# docopt==0.6.2\n",
    "# dotenv==0.9.9\n",
    "# en_core_web_lg==3.8.0\n",
    "# et_xmlfile==2.0.0\n",
    "# exceptiongroup==1.3.1\n",
    "# executing==2.2.1\n",
    "# fief-client==0.20.0\n",
    "# filelock==3.16.1\n",
    "# fonttools==4.55.0\n",
    "# frozenlist==1.5.0\n",
    "# fsspec==2024.9.0\n",
    "# h11==0.14.0\n",
    "# hf-xet==1.2.0\n",
    "# httpcore==1.0.7\n",
    "# httpx==0.27.2\n",
    "# huggingface-hub==0.26.3\n",
    "# idna==3.10\n",
    "# imageio==2.36.1\n",
    "# importlib_metadata==8.7.0\n",
    "# intervaltree==3.1.0\n",
    "# ipykernel==7.1.0\n",
    "# ipymarkup==0.9.0\n",
    "# ipython==8.37.0\n",
    "# jedi==0.19.2\n",
    "# Jinja2==3.1.4\n",
    "# jiter==0.12.0\n",
    "# joblib==1.4.2\n",
    "# jupyter_client==8.6.3\n",
    "# jupyter_core==5.9.1\n",
    "# jwcrypto==1.5.6\n",
    "# kiwisolver==1.4.7\n",
    "# langcodes==3.5.0\n",
    "# language_data==1.3.0\n",
    "# lazy_loader==0.4\n",
    "# lime==0.2.0.1\n",
    "# llvmlite==0.43.0\n",
    "# logging==0.4.9.6\n",
    "# marisa-trie==1.2.1\n",
    "# markdown-it-py==3.0.0\n",
    "# MarkupSafe==3.0.2\n",
    "# matplotlib==3.9.2\n",
    "# matplotlib-inline==0.2.1\n",
    "# mdurl==0.1.2\n",
    "# mpmath==1.3.0\n",
    "# multidict==6.1.0\n",
    "# multiprocess==0.70.16\n",
    "# murmurhash==1.0.11\n",
    "# natasha==1.6.0\n",
    "# navec==0.10.0\n",
    "# nest_asyncio==1.6.0\n",
    "# networkx==3.4.2\n",
    "# nltk==3.9.2\n",
    "# numba==0.60.0\n",
    "# numexpr==2.14.1\n",
    "# numpy==2.0.2\n",
    "# openai==2.9.0\n",
    "# openpyxl==3.1.5\n",
    "# packaging==24.2\n",
    "# pandas==2.3.3\n",
    "# parso==0.8.5\n",
    "# pexpect==4.9.0\n",
    "# pickleshare==0.7.5\n",
    "# pillow==11.0.0\n",
    "# pip==25.3\n",
    "# platformdirs==4.5.1\n",
    "# preshed==3.0.9\n",
    "# prometheus_client==0.21.0\n",
    "# prompt-toolkit==3.0.36\n",
    "# propcache==0.2.0\n",
    "# psutil==6.1.0\n",
    "# ptyprocess==0.7.0\n",
    "# pure_eval==0.2.3\n",
    "# py-cpuinfo==9.0.0\n",
    "# pyarrow==18.1.0\n",
    "# pycparser==2.22\n",
    "# pydantic==2.10.2\n",
    "# pydantic_core==2.27.1\n",
    "# Pygments==2.18.0\n",
    "# pymorphy2==0.9.1\n",
    "# pymorphy2-dicts-ru==2.4.417127.4579844\n",
    "# pymorphy3==2.0.6\n",
    "# pymorphy3-dicts-ru==2.4.417150.4580142\n",
    "# pynvml==11.5.3\n",
    "# pyparsing==3.2.0\n",
    "# python-dateutil==2.9.0.post0\n",
    "# python-dotenv==1.2.1\n",
    "# pytz==2024.2\n",
    "# PyYAML==6.0.2\n",
    "# pyzmq==27.1.0\n",
    "# questionary==2.0.1\n",
    "# RapidFuzz==3.10.1\n",
    "# razdel==0.5.0\n",
    "# regex==2024.11.6\n",
    "# requests==2.32.3\n",
    "# rich==13.9.4\n",
    "# ru_core_news_lg==3.8.0\n",
    "# russian-paraphrasers==0.0.3\n",
    "# safetensors==0.4.5\n",
    "# scikit-image==0.24.0\n",
    "# scikit-learn==1.6.0rc1\n",
    "# scipy==1.14.1\n",
    "# seaborn==0.13.2\n",
    "# sentence-transformers==0.4.0\n",
    "# sentencepiece==0.2.1\n",
    "# setuptools==75.6.0\n",
    "# shap==0.46.0\n",
    "# shellingham==1.5.4\n",
    "# six==1.16.0\n",
    "# slicer==0.0.8\n",
    "# slovnet==0.6.0\n",
    "# smart-open==6.4.0\n",
    "# sniffio==1.3.1\n",
    "# sortedcontainers==2.4.0\n",
    "# soupsieve==2.8\n",
    "# spacy==3.8.2\n",
    "# spacy-legacy==3.0.12\n",
    "# spacy-loggers==1.0.5\n",
    "# srsly==2.4.8\n",
    "# stack_data==0.6.3\n",
    "# sympy==1.13.1\n",
    "# termcolor==2.3.0\n",
    "# thinc==8.3.2\n",
    "# threadpoolctl==3.5.0\n",
    "# tifffile==2024.9.20\n",
    "# tokenizers==0.20.3\n",
    "# torch==2.5.1\n",
    "# torchvision==0.20.1\n",
    "# tornado==6.5.1\n",
    "# tqdm==4.67.1\n",
    "# traitlets==5.14.3\n",
    "# transformers==4.46.3\n",
    "# typer==0.9.4\n",
    "# types-python-dateutil==2.9.0.20241003\n",
    "# typing_extensions==4.12.2\n",
    "# tzdata==2024.2\n",
    "# urllib3==2.2.3\n",
    "# wasabi==1.1.3\n",
    "# wcwidth==0.2.13\n",
    "# weasel==0.3.4\n",
    "# wheel==0.45.1\n",
    "# wordcloud==1.9.4\n",
    "# xxhash==3.5.0\n",
    "# yargy==0.16.0\n",
    "# yarl==1.18.0\n",
    "# yaspin==3.1.0\n",
    "# zipp==3.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005348eb-2878-4eaf-8aa8-f2fb330a64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "REPO_DIR = Path(\"/tmp/HEARTS-Text-Stereotype-Detection\").resolve()\n",
    "DATA_DIR = REPO_DIR / \"Model Training and Evaluation\"\n",
    "\n",
    "# Change working directory to the repo\n",
    "os.chdir(REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619d26e8-fea7-41df-a0c7-2695849aa57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /tmp/HEARTS-Text-Stereotype-Detection\n",
      "Repository directory: /tmp/HEARTS-Text-Stereotype-Detection\n",
      "Data directory: /tmp/HEARTS-Text-Stereotype-Detection/Model Training and Evaluation\n",
      "Python version: 3.9.21 (main, Aug 19 2025, 00:00:00) \n",
      "[GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "Platform: Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3090 Ti\n",
      "Transformers version: 4.57.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working dir:\", Path.cwd())\n",
    "print(\"Repository directory:\", REPO_DIR)\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635eb6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking dataset files:\n",
      "\n",
      "MGSD.csv                                      -> FOUND\n",
      "Winoqueer - GPT Augmentation.csv              -> FOUND\n",
      "SeeGULL - GPT Augmentation.csv                -> FOUND\n",
      "../Exploratory Data Analysis/MGSD - Expanded.csv -> FOUND\n"
     ]
    }
   ],
   "source": [
    "# Check the csv files\n",
    "csv_files = [\n",
    "    \"MGSD.csv\",\n",
    "    \"Winoqueer - GPT Augmentation.csv\",\n",
    "    \"SeeGULL - GPT Augmentation.csv\",\n",
    "\n",
    "    # MGSD Expanded is located elsewhere\n",
    "    \"../Exploratory Data Analysis/MGSD - Expanded.csv\",\n",
    "]\n",
    "\n",
    "print(\"\\nChecking dataset files:\\n\")\n",
    "for f in csv_files:\n",
    "    path = (DATA_DIR / f).resolve()\n",
    "    print(f\"{f:45} -> {'FOUND' if path.exists() else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f71d69",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 3:}$ Reproduce baseline results within ±5% of original paper metrics \n",
    "\n",
    "To maintain consistency with the HEARTS baseline, all experiments were conducted using a fixed random seed across Python, NumPy, and PyTorch (both CPU and CUDA). A dedicated module `BERT_Models_Fine_Tuning_Replication.py` was duplicated and used from the original `BERT_Models_Fine_Tuning.py`, with minimal fixes to file paths, training arguments, and compatibility issues, and a single entry-point function.\n",
    "\n",
    "The `run_full_hearts_pipeline()` function:\n",
    "\n",
    "1. Trains ALBERT, BERT, and DistilBERT on each of the four training settings\n",
    "(mgsd_trained, winoqueer_gpt_augmentation_trained,\n",
    "seegull_gpt_augmentation_trained,\n",
    "merged_winoqueer_seegull_gpt_augmentation_trained), and\n",
    "\n",
    "2. Evaluates each model on the four evaluation sets\n",
    "(mgsd, winoqueer_gpt_augmentation,\n",
    "seegull_gpt_augmentation,\n",
    "merged_winoqueer_seegull_gpt_augmentation).\n",
    "\n",
    "For each configuration, the accuracy, macro-precision, macro-recall, and macro-F1 scores are recorded in a CSV file and summarised in a compact results table. The accuracy and macro-F1 scores achieved in this study for all three model families and most dataset combinations are within approximately $±5$ percentage points of the metrics reported in the HEARTS paper, thereby meeting the baseline replication requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ff1ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f6019-7808-4471-9559-96468a946c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /tmp/HEARTS-Text-Stereotype-Detection/Model Training and Evaluation/MGSD.csv\n",
      "First few examples from the training data:\n",
      "        group                                               text  label  \\\n",
      "38518    race  Syria is a well known area nowadays. There is ...      1   \n",
      "12979    race                   The people of germany are angry.      1   \n",
      "35494    race  Lebanon is a small country in the Middle East ...      1   \n",
      "42213    race  We have a french tutor. She is very humble and...      0   \n",
      "16293  gender                              Every male acts dumb.      1   \n",
      "\n",
      "      data_name  \n",
      "38518      MGSD  \n",
      "12979      MGSD  \n",
      "35494      MGSD  \n",
      "42213      MGSD  \n",
      "16293      MGSD  \n",
      "First few examples from the testing data:\n",
      "            group                                               text  label  \\\n",
      "17851        race      When i see an arab, i think they are a happy.      0   \n",
      "37449        race  People in India dress differently. The types o...      0   \n",
      "23634      gender    Everyone has a grandmother. The bridge is long.      0   \n",
      "36047        race  Cameroon is a country. It is a popular tourist...      0   \n",
      "28528  profession  The delivery man drove a truck. He was highly ...      0   \n",
      "\n",
      "      data_name  \n",
      "17851      MGSD  \n",
      "37449      MGSD  \n",
      "23634      MGSD  \n",
      "36047      MGSD  \n",
      "28528      MGSD  \n",
      "Train data size: 41493\n",
      "Test data size: 10374\n",
      "Loading: /tmp/HEARTS-Text-Stereotype-Detection/Model Training and Evaluation/Winoqueer - GPT Augmentation.csv\n",
      "First few examples from the training data:\n",
      "                                                   text  label        group  \\\n",
      "68                            Gay people are survivors.      1          Gay   \n",
      "2269                      Transgender people are doors.      0  Transgender   \n",
      "3208  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "826                           most lesbians are sinful.      1      Lesbian   \n",
      "2452   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                         data_name  \n",
      "68    Winoqueer - GPT Augmentation  \n",
      "2269  Winoqueer - GPT Augmentation  \n",
      "3208  Winoqueer - GPT Augmentation  \n",
      "826   Winoqueer - GPT Augmentation  \n",
      "2452  Winoqueer - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label        group  \\\n",
      "3236                  most people who are Gay are kites      0          Gay   \n",
      "2749            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2223                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "2724  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "1506  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                         data_name  \n",
      "3236  Winoqueer - GPT Augmentation  \n",
      "2749  Winoqueer - GPT Augmentation  \n",
      "2223  Winoqueer - GPT Augmentation  \n",
      "2724  Winoqueer - GPT Augmentation  \n",
      "1506  Winoqueer - GPT Augmentation  \n",
      "Train data size: 2611\n",
      "Test data size: 653\n",
      "Loading: /tmp/HEARTS-Text-Stereotype-Detection/Model Training and Evaluation/SeeGULL - GPT Augmentation.csv\n",
      "First few examples from the training data:\n",
      "                                                   text  label          group  \\\n",
      "1206  The Russian cautious approach to learning affe...      0        Russian   \n",
      "213    The United States tourist was rude to the locals      1  United States   \n",
      "1165  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "76    The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "707     The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                       data_name  \n",
      "1206  SeeGULL - GPT Augmentation  \n",
      "213   SeeGULL - GPT Augmentation  \n",
      "1165  SeeGULL - GPT Augmentation  \n",
      "76    SeeGULL - GPT Augmentation  \n",
      "707   SeeGULL - GPT Augmentation  \n",
      "First few examples from the testing data:\n",
      "                                                   text  label    group  \\\n",
      "840   The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1956  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "348   Barundians are often criticized for their unre...      1  Barundi   \n",
      "1735             The Afghans stars had to beg for music      0  Afghans   \n",
      "1913  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                       data_name  \n",
      "840   SeeGULL - GPT Augmentation  \n",
      "1956  SeeGULL - GPT Augmentation  \n",
      "348   SeeGULL - GPT Augmentation  \n",
      "1735  SeeGULL - GPT Augmentation  \n",
      "1913  SeeGULL - GPT Augmentation  \n",
      "Train data size: 1656\n",
      "Test data size: 414\n",
      "First few examples from merged training data:\n",
      "                                                text  label        group  \\\n",
      "0                          Gay people are survivors.      1          Gay   \n",
      "1                      Transgender people are doors.      0  Transgender   \n",
      "2  most people who are Pansexual are undermined i...      0    Pansexual   \n",
      "3                          most lesbians are sinful.      1      Lesbian   \n",
      "4   All LGBTQ people are unfit for spaceship travel.      0        LGBTQ   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label        group  \\\n",
      "0                  most people who are Gay are kites      0          Gay   \n",
      "1            we expect that LGBTQ people are paints.      0        LGBTQ   \n",
      "2                      LGBTQ people are televisions.      0        LGBTQ   \n",
      "3  we expect that LGBTQ people are unfit for moon...      0        LGBTQ   \n",
      "4  all Transgender people face mislabeling but ar...      0  Transgender   \n",
      "\n",
      "                      data_name  \n",
      "0  Winoqueer - GPT Augmentation  \n",
      "1  Winoqueer - GPT Augmentation  \n",
      "2  Winoqueer - GPT Augmentation  \n",
      "3  Winoqueer - GPT Augmentation  \n",
      "4  Winoqueer - GPT Augmentation  \n",
      "Train data merged size: 44104\n",
      "Test data merged size: 11027\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 43149\n",
      "Test data merged size: 10788\n",
      "First few examples from merged training data:\n",
      "                                                text  label          group  \\\n",
      "0  The Russian cautious approach to learning affe...      0        Russian   \n",
      "1   The United States tourist was rude to the locals      1  United States   \n",
      "2  Ethiopian challenges were a symptom of economi...      0      Ethiopian   \n",
      "3  The Vietnamese factory workers were subjected ...      1     Vietnamese   \n",
      "4    The Mexican leader managed a large organization      0        Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "First few examples from merged testing data:\n",
      "                                                text  label    group  \\\n",
      "0  The Somali refugees were labeled as unacquaint...      0  Somalis   \n",
      "1  The Swiss man was considered spaghetti by his ...      0    Swiss   \n",
      "2  Barundians are often criticized for their unre...      1  Barundi   \n",
      "3             The Afghans stars had to beg for music      0  Afghans   \n",
      "4  The Mexican popcorn popped energetically throu...      0  Mexican   \n",
      "\n",
      "                    data_name  \n",
      "0  SeeGULL - GPT Augmentation  \n",
      "1  SeeGULL - GPT Augmentation  \n",
      "2  SeeGULL - GPT Augmentation  \n",
      "3  SeeGULL - GPT Augmentation  \n",
      "4  SeeGULL - GPT Augmentation  \n",
      "Train data merged size: 45760\n",
      "Test data merged size: 11441\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/spiece.model\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:01<00:00, 24264.01 examples/s]\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:01<00:00, 30903.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [2, 57, 42, 462, 798, 737, 377, 13479, 60, 31, 376, 21, 78, 2146, 16, 23358, 18, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 25066.94 examples/s]\n",
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 25612.81 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'group': 'race', 'text': 'My friend is German and sometimes we hand out. He gets very angry sometimes.', 'label': 1, 'data_name': 'MGSD', '__index_level_0__': 36651, 'input_ids': [2, 51, 860, 25, 548, 17, 1030, 95, 224, 70, 9, 24, 3049, 253, 3365, 1030, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 33,194\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,114\n",
      "  Number of trainable parameters = 11,685,122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3114' max='3114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3114/3114 08:46, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.421646</td>\n",
       "      <td>0.762911</td>\n",
       "      <td>0.759820</td>\n",
       "      <td>0.761298</td>\n",
       "      <td>0.759820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.391999</td>\n",
       "      <td>0.806277</td>\n",
       "      <td>0.771857</td>\n",
       "      <td>0.783901</td>\n",
       "      <td>0.771857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.417022</td>\n",
       "      <td>0.807062</td>\n",
       "      <td>0.786549</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.786549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.474694</td>\n",
       "      <td>0.804454</td>\n",
       "      <td>0.798714</td>\n",
       "      <td>0.801402</td>\n",
       "      <td>0.798714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.675297</td>\n",
       "      <td>0.803132</td>\n",
       "      <td>0.781201</td>\n",
       "      <td>0.789853</td>\n",
       "      <td>0.781201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.788018</td>\n",
       "      <td>0.801895</td>\n",
       "      <td>0.795548</td>\n",
       "      <td>0.798498</td>\n",
       "      <td>0.795548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-519\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-519/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-519/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-519/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-519/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-1038\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-1038/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-1038/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-1038/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-1038/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/mgsd_trained/checkpoint-519] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-1557\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-1557/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-1557/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-1557/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-1557/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-2076\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-2076/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-2076/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-2076/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-2076/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/mgsd_trained/checkpoint-1557] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-2595\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-2595/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-2595/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-2595/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-2595/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/mgsd_trained/checkpoint-2076] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained/checkpoint-3114\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/checkpoint-3114/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/checkpoint-3114/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/checkpoint-3114/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/checkpoint-3114/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/mgsd_trained/checkpoint-2595] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2/mgsd_trained/checkpoint-1038 (score: 0.39199861884117126).\n",
      "Deleting older checkpoint [model_output_albertv2/mgsd_trained/checkpoint-3114] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_albertv2/mgsd_trained\n",
      "Configuration saved in model_output_albertv2/mgsd_trained/config.json\n",
      "Model weights saved in model_output_albertv2/mgsd_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/mgsd_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/mgsd_trained/special_tokens_map.json\n",
      "loading configuration file model_output_albertv2/mgsd_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/mgsd_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_albertv2/mgsd_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32979.49 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32939.82 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/mgsd_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/mgsd_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/mgsd_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/mgsd_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 26623.15 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30635.88 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_albertv2/mgsd_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/mgsd_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/mgsd_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/mgsd_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 25262.18 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27014.61 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/mgsd_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/mgsd_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/mgsd_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/mgsd_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 25754.99 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29745.51 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/spiece.model\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 39269.76 examples/s]\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 35202.47 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [2, 95, 4186, 30, 538, 4772, 1857, 148, 50, 70, 20, 1891, 66, 4146, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 33515.47 examples/s]\n",
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 31945.78 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'all lesbians are studious.', 'label': 0, 'group': 'Lesbian', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 2716, 'input_ids': [2, 65, 10564, 18, 50, 1120, 267, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198\n",
      "  Number of trainable parameters = 11,685,122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:14, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406159</td>\n",
       "      <td>0.871846</td>\n",
       "      <td>0.795361</td>\n",
       "      <td>0.819554</td>\n",
       "      <td>0.795361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.200735</td>\n",
       "      <td>0.925505</td>\n",
       "      <td>0.899788</td>\n",
       "      <td>0.911097</td>\n",
       "      <td>0.899788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.078651</td>\n",
       "      <td>0.974940</td>\n",
       "      <td>0.971805</td>\n",
       "      <td>0.973351</td>\n",
       "      <td>0.971805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.979424</td>\n",
       "      <td>0.985560</td>\n",
       "      <td>0.982406</td>\n",
       "      <td>0.985560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.057458</td>\n",
       "      <td>0.982292</td>\n",
       "      <td>0.982292</td>\n",
       "      <td>0.982292</td>\n",
       "      <td>0.982292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.980816</td>\n",
       "      <td>0.983926</td>\n",
       "      <td>0.982350</td>\n",
       "      <td>0.983926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-33] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-66] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-165] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-132 (score: 0.05476154759526253).\n",
      "Deleting older checkpoint [model_output_albertv2/winoqueer_gpt_augmentation_trained/checkpoint-198] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_albertv2/winoqueer_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/winoqueer_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_albertv2/winoqueer_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32616.59 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32063.69 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 26737.94 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30124.07 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 24882.03 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 26299.02 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/winoqueer_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/winoqueer_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 26019.70 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29392.62 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/spiece.model\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 32692.57 examples/s]\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 34885.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [2, 14, 862, 8708, 806, 1587, 25, 8904, 2428, 20, 885, 1166, 3, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 28725.74 examples/s]\n",
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 29558.04 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'The Ethiopian woman was short and petite', 'label': 1, 'group': 'Ethiopian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 609, 'input_ids': [2, 14, 16659, 524, 23, 502, 17, 22471, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,324\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 126\n",
      "  Number of trainable parameters = 11,685,122\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:07, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496939</td>\n",
       "      <td>0.810158</td>\n",
       "      <td>0.650850</td>\n",
       "      <td>0.658717</td>\n",
       "      <td>0.650850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278642</td>\n",
       "      <td>0.872588</td>\n",
       "      <td>0.869186</td>\n",
       "      <td>0.870844</td>\n",
       "      <td>0.869186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.291309</td>\n",
       "      <td>0.867118</td>\n",
       "      <td>0.896091</td>\n",
       "      <td>0.876893</td>\n",
       "      <td>0.896091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.279223</td>\n",
       "      <td>0.883646</td>\n",
       "      <td>0.900677</td>\n",
       "      <td>0.890901</td>\n",
       "      <td>0.900677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.261838</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.907484</td>\n",
       "      <td>0.902943</td>\n",
       "      <td>0.907484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.265918</td>\n",
       "      <td>0.895170</td>\n",
       "      <td>0.905222</td>\n",
       "      <td>0.899807</td>\n",
       "      <td>0.905222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-21] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-63] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-42] due to args.save_total_limit\n",
      "Deleting older checkpoint [model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-84] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-105 (score: 0.2618383467197418).\n",
      "Deleting older checkpoint [model_output_albertv2/seegull_gpt_augmentation_trained/checkpoint-126] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_albertv2/seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2/seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_albertv2/seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_albertv2/seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32752.72 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32542.19 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 26535.68 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31447.60 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 25123.87 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 26863.03 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 25824.60 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29088.25 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/spiece.model\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--albert--albert-base-v2/snapshots/8e2f239c5f8a2c0f253781ca60135db913e5c80c/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 23008.99 examples/s]\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 31414.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [2, 14, 1647, 110, 494, 14, 3994, 23, 7927, 43, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 25961.68 examples/s]\n",
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 26573.65 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The following columns in the Training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 36,608\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,432\n",
      "  Number of trainable parameters = 11,685,122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'we expect that Queer people are artistic.', 'label': 0, 'group': 'Queer', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3705, 'input_ids': [2, 95, 4186, 30, 20242, 148, 50, 4917, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3432' max='3432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3432/3432 08:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.392600</td>\n",
       "      <td>0.404256</td>\n",
       "      <td>0.781526</td>\n",
       "      <td>0.756578</td>\n",
       "      <td>0.765872</td>\n",
       "      <td>0.756578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.373967</td>\n",
       "      <td>0.808086</td>\n",
       "      <td>0.800762</td>\n",
       "      <td>0.804144</td>\n",
       "      <td>0.800762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.388171</td>\n",
       "      <td>0.821823</td>\n",
       "      <td>0.807886</td>\n",
       "      <td>0.813955</td>\n",
       "      <td>0.807886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.438449</td>\n",
       "      <td>0.819380</td>\n",
       "      <td>0.810596</td>\n",
       "      <td>0.814610</td>\n",
       "      <td>0.810596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.606645</td>\n",
       "      <td>0.809638</td>\n",
       "      <td>0.812734</td>\n",
       "      <td>0.811131</td>\n",
       "      <td>0.812734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.738903</td>\n",
       "      <td>0.808658</td>\n",
       "      <td>0.808987</td>\n",
       "      <td>0.808822</td>\n",
       "      <td>0.808987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144 (score: 0.37396711111068726).\n",
      "Deleting older checkpoint [model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32820.62 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 33051.52 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [2, 127, 148, 72, 50, 3398, 50, 19196, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 26342.45 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30666.18 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 25022.22 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 26777.21 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [2, 76, 31, 196, 40, 3666, 15, 31, 277, 59, 50, 21, 1700, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 25863.35 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29440.71 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [2, 14, 16330, 8790, 46, 14348, 28, 367, 26692, 108, 1427, 34, 109, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Saved full results to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:01<00:00, 28775.37 examples/s]\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:00<00:00, 36683.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 31421.08 examples/s]\n",
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 26846.98 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'group': 'race', 'text': 'My friend is German and sometimes we hand out. He gets very angry sometimes.', 'label': 1, 'data_name': 'MGSD', '__index_level_0__': 36651, 'input_ids': [101, 2026, 2767, 2003, 2446, 1998, 2823, 2057, 2192, 2041, 1012, 2002, 4152, 2200, 4854, 2823, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 33,194\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,114\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3114' max='3114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3114/3114 03:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.419942</td>\n",
       "      <td>0.768544</td>\n",
       "      <td>0.768677</td>\n",
       "      <td>0.768610</td>\n",
       "      <td>0.768677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>0.394085</td>\n",
       "      <td>0.795470</td>\n",
       "      <td>0.785017</td>\n",
       "      <td>0.789643</td>\n",
       "      <td>0.785017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.422996</td>\n",
       "      <td>0.792217</td>\n",
       "      <td>0.786708</td>\n",
       "      <td>0.789284</td>\n",
       "      <td>0.786708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.455620</td>\n",
       "      <td>0.801655</td>\n",
       "      <td>0.796885</td>\n",
       "      <td>0.799141</td>\n",
       "      <td>0.796885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.503743</td>\n",
       "      <td>0.802036</td>\n",
       "      <td>0.801280</td>\n",
       "      <td>0.801655</td>\n",
       "      <td>0.801280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.528433</td>\n",
       "      <td>0.802224</td>\n",
       "      <td>0.806487</td>\n",
       "      <td>0.804241</td>\n",
       "      <td>0.806487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-519\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-519/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-519/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-519/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-519/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-1038\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-1038/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-1038/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-1038/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-1038/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/mgsd_trained/checkpoint-519] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-1557\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-1557/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-1557/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-1557/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-1557/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-2076\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-2076/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-2076/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-2076/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-2076/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/mgsd_trained/checkpoint-1557] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-2595\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-2595/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-2595/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-2595/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-2595/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/mgsd_trained/checkpoint-2076] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained/checkpoint-3114\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/checkpoint-3114/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/checkpoint-3114/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/checkpoint-3114/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/checkpoint-3114/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/mgsd_trained/checkpoint-2595] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_distilbert/mgsd_trained/checkpoint-1038 (score: 0.39408475160598755).\n",
      "Deleting older checkpoint [model_output_distilbert/mgsd_trained/checkpoint-3114] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_distilbert/mgsd_trained\n",
      "Configuration saved in model_output_distilbert/mgsd_trained/config.json\n",
      "Model weights saved in model_output_distilbert/mgsd_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/mgsd_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/mgsd_trained/special_tokens_map.json\n",
      "loading configuration file model_output_distilbert/mgsd_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_distilbert/mgsd_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 36817.36 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 36790.66 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/mgsd_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/mgsd_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/mgsd_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 33828.33 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 34445.69 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_distilbert/mgsd_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/mgsd_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/mgsd_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 31346.42 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 39870.79 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/mgsd_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/mgsd_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/mgsd_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 32204.35 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 45824.66 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 43712.69 examples/s]\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 13366.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 23435.62 examples/s]\n",
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 33046.41 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'all lesbians are studious.', 'label': 0, 'group': 'Lesbian', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 2716, 'input_ids': [101, 2035, 11690, 2015, 2024, 2996, 2271, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:09, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.255498</td>\n",
       "      <td>0.898562</td>\n",
       "      <td>0.891883</td>\n",
       "      <td>0.895099</td>\n",
       "      <td>0.891883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.130537</td>\n",
       "      <td>0.953198</td>\n",
       "      <td>0.958998</td>\n",
       "      <td>0.956015</td>\n",
       "      <td>0.958998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>0.098093</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.961791</td>\n",
       "      <td>0.960287</td>\n",
       "      <td>0.961791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.071885</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.976706</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.976706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.067363</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.976706</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.976706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.063389</td>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.976706</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.976706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-33] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-66] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-132] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-165] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_distilbert/winoqueer_gpt_augmentation_trained/checkpoint-198 (score: 0.06338918209075928).\n",
      "Saving model checkpoint to model_output_distilbert/winoqueer_gpt_augmentation_trained\n",
      "Configuration saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/winoqueer_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_distilbert/winoqueer_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 37391.37 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 37079.54 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 34003.21 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 33467.13 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 31307.60 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 40814.33 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/winoqueer_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/winoqueer_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 24977.79 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 45280.82 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 40238.67 examples/s]\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 39438.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 35049.31 examples/s]\n",
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 33797.12 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'The Ethiopian woman was short and petite', 'label': 1, 'group': 'Ethiopian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 609, 'input_ids': [101, 1996, 15101, 2450, 2001, 2460, 1998, 20146, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,324\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 126\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:06, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568993</td>\n",
       "      <td>0.332831</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.399638</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321311</td>\n",
       "      <td>0.847768</td>\n",
       "      <td>0.866822</td>\n",
       "      <td>0.855378</td>\n",
       "      <td>0.866822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.864496</td>\n",
       "      <td>0.896070</td>\n",
       "      <td>0.874290</td>\n",
       "      <td>0.896070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.276468</td>\n",
       "      <td>0.865551</td>\n",
       "      <td>0.900554</td>\n",
       "      <td>0.875095</td>\n",
       "      <td>0.900554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.227934</td>\n",
       "      <td>0.902528</td>\n",
       "      <td>0.923220</td>\n",
       "      <td>0.911085</td>\n",
       "      <td>0.923220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.241385</td>\n",
       "      <td>0.895450</td>\n",
       "      <td>0.920937</td>\n",
       "      <td>0.905213</td>\n",
       "      <td>0.920937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-21] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-42] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-63] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-84] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-105 (score: 0.22793379426002502).\n",
      "Deleting older checkpoint [model_output_distilbert/seegull_gpt_augmentation_trained/checkpoint-126] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_distilbert/seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_distilbert/seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_distilbert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_distilbert/seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 36814.89 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 36601.37 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 33272.18 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 33910.91 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 31216.77 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 41061.68 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 24997.10 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 44625.57 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 29421.41 examples/s]\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 33418.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 31388.93 examples/s]\n",
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 28018.19 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'we expect that Queer people are artistic.', 'label': 0, 'group': 'Queer', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3705, 'input_ids': [101, 2057, 5987, 2008, 19483, 2111, 2024, 6018, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 36,608\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,432\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3432' max='3432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3432/3432 03:52, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.402048</td>\n",
       "      <td>0.775252</td>\n",
       "      <td>0.789526</td>\n",
       "      <td>0.780651</td>\n",
       "      <td>0.789526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.371610</td>\n",
       "      <td>0.805155</td>\n",
       "      <td>0.811742</td>\n",
       "      <td>0.808178</td>\n",
       "      <td>0.811742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.392711</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.818871</td>\n",
       "      <td>0.811804</td>\n",
       "      <td>0.818871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.434471</td>\n",
       "      <td>0.811132</td>\n",
       "      <td>0.820265</td>\n",
       "      <td>0.815176</td>\n",
       "      <td>0.820265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.480236</td>\n",
       "      <td>0.812676</td>\n",
       "      <td>0.824637</td>\n",
       "      <td>0.817728</td>\n",
       "      <td>0.824637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.513283</td>\n",
       "      <td>0.814352</td>\n",
       "      <td>0.821738</td>\n",
       "      <td>0.817718</td>\n",
       "      <td>0.821738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144 (score: 0.3716101348400116).\n",
      "Deleting older checkpoint [model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 38347.86 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 35948.50 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 34410.20 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 34705.93 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 31747.00 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 39195.75 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 32604.98 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 44820.72 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Saved full results to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_distilbert/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:01<00:00, 23762.39 examples/s]\n",
      "Map: 100%|██████████████████████| 33194/33194 [00:01<00:00, 31126.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'group': 'religion', 'text': 'Have you ever met someone named Mohammed? I need a new pair of pajamas.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 36953, 'input_ids': [101, 2031, 2017, 2412, 2777, 2619, 2315, 12619, 1029, 1045, 2342, 1037, 2047, 3940, 1997, 27621, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 28170.47 examples/s]\n",
      "Map: 100%|████████████████████████| 8299/8299 [00:00<00:00, 25554.07 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'group': 'race', 'text': 'My friend is German and sometimes we hand out. He gets very angry sometimes.', 'label': 1, 'data_name': 'MGSD', '__index_level_0__': 36651, 'input_ids': [101, 2026, 2767, 2003, 2446, 1998, 2823, 2057, 2192, 2041, 1012, 2002, 4152, 2200, 4854, 2823, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 33,194\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,114\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3114' max='3114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3114/3114 07:44, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.386370</td>\n",
       "      <td>0.786611</td>\n",
       "      <td>0.786230</td>\n",
       "      <td>0.786420</td>\n",
       "      <td>0.786230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.364428</td>\n",
       "      <td>0.818632</td>\n",
       "      <td>0.803181</td>\n",
       "      <td>0.809778</td>\n",
       "      <td>0.803181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.415161</td>\n",
       "      <td>0.822286</td>\n",
       "      <td>0.811744</td>\n",
       "      <td>0.816476</td>\n",
       "      <td>0.811744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.462410</td>\n",
       "      <td>0.814637</td>\n",
       "      <td>0.825875</td>\n",
       "      <td>0.819431</td>\n",
       "      <td>0.825875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.546776</td>\n",
       "      <td>0.818686</td>\n",
       "      <td>0.827222</td>\n",
       "      <td>0.822507</td>\n",
       "      <td>0.827222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.591007</td>\n",
       "      <td>0.814922</td>\n",
       "      <td>0.823312</td>\n",
       "      <td>0.818678</td>\n",
       "      <td>0.823312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-519\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-519/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-519/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-519/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-519/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-1038\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-1038/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-1038/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-1038/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-1038/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/mgsd_trained/checkpoint-519] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-1557\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-1557/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-1557/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-1557/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-1557/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-2076\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-2076/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-2076/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-2076/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-2076/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/mgsd_trained/checkpoint-1557] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-2595\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-2595/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-2595/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-2595/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-2595/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/mgsd_trained/checkpoint-2076] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8299\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained/checkpoint-3114\n",
      "Configuration saved in model_output_bert/mgsd_trained/checkpoint-3114/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/checkpoint-3114/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/checkpoint-3114/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/checkpoint-3114/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/mgsd_trained/checkpoint-2595] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_bert/mgsd_trained/checkpoint-1038 (score: 0.3644278049468994).\n",
      "Deleting older checkpoint [model_output_bert/mgsd_trained/checkpoint-3114] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_bert/mgsd_trained\n",
      "Configuration saved in model_output_bert/mgsd_trained/config.json\n",
      "Model weights saved in model_output_bert/mgsd_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/mgsd_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/mgsd_trained/special_tokens_map.json\n",
      "loading configuration file model_output_bert/mgsd_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_bert/mgsd_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 33534.30 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 32467.73 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/mgsd_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/mgsd_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/mgsd_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30932.22 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30669.97 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_bert/mgsd_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/mgsd_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/mgsd_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27824.77 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27138.07 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/mgsd_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/mgsd_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/mgsd_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_bert/mgsd_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29222.57 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29499.71 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Saved full results to: result_output_bert/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/mgsd_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 40277.73 examples/s]\n",
      "Map: 100%|████████████████████████| 2088/2088 [00:00<00:00, 35950.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'we expect that nonbinary people are out to share their views.', 'label': 0, 'group': 'NB', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 1788, 'input_ids': [101, 2057, 5987, 2008, 2512, 21114, 2854, 2111, 2024, 2041, 2000, 3745, 2037, 5328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 35062.03 examples/s]\n",
      "Map: 100%|██████████████████████████| 523/523 [00:00<00:00, 32251.54 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'all lesbians are studious.', 'label': 0, 'group': 'Lesbian', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 2716, 'input_ids': [101, 2035, 11690, 2015, 2024, 2996, 2271, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2,088\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:17, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.253598</td>\n",
       "      <td>0.917860</td>\n",
       "      <td>0.907957</td>\n",
       "      <td>0.912658</td>\n",
       "      <td>0.907957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.398300</td>\n",
       "      <td>0.096847</td>\n",
       "      <td>0.967516</td>\n",
       "      <td>0.965981</td>\n",
       "      <td>0.966743</td>\n",
       "      <td>0.965981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398300</td>\n",
       "      <td>0.050443</td>\n",
       "      <td>0.982302</td>\n",
       "      <td>0.986956</td>\n",
       "      <td>0.984581</td>\n",
       "      <td>0.986956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.033483</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.989021</td>\n",
       "      <td>0.993017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.038840</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.989021</td>\n",
       "      <td>0.993017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.037719</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.989021</td>\n",
       "      <td>0.993017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-33] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-66] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 523\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-165] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-132 (score: 0.03348271921277046).\n",
      "Deleting older checkpoint [model_output_bert/winoqueer_gpt_augmentation_trained/checkpoint-198] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_bert/winoqueer_gpt_augmentation_trained\n",
      "Configuration saved in model_output_bert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_bert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/winoqueer_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/winoqueer_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_bert/winoqueer_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 36305.42 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 33955.45 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/winoqueer_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31383.94 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31320.54 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/winoqueer_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 28386.71 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27367.28 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/winoqueer_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/winoqueer_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/winoqueer_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_bert/winoqueer_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29391.16 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29863.43 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/winoqueer_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 37434.75 examples/s]\n",
      "Map: 100%|████████████████████████| 1324/1324 [00:00<00:00, 35504.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The average Chadian income is modest compared to developed countries', 'label': 0, 'group': 'Chadian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 1249, 'input_ids': [101, 1996, 2779, 9796, 2937, 3318, 2003, 10754, 4102, 2000, 2764, 3032, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 32066.99 examples/s]\n",
      "Map: 100%|██████████████████████████| 332/332 [00:00<00:00, 27773.52 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'The Ethiopian woman was short and petite', 'label': 1, 'group': 'Ethiopian', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 609, 'input_ids': [101, 1996, 15101, 2450, 2001, 2460, 1998, 20146, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,324\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 126\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:11, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.627826</td>\n",
       "      <td>0.332831</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.399638</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505917</td>\n",
       "      <td>0.810927</td>\n",
       "      <td>0.614834</td>\n",
       "      <td>0.608315</td>\n",
       "      <td>0.614834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.355087</td>\n",
       "      <td>0.843460</td>\n",
       "      <td>0.880213</td>\n",
       "      <td>0.850821</td>\n",
       "      <td>0.880213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.261053</td>\n",
       "      <td>0.876640</td>\n",
       "      <td>0.907362</td>\n",
       "      <td>0.886805</td>\n",
       "      <td>0.907362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.233960</td>\n",
       "      <td>0.889153</td>\n",
       "      <td>0.920896</td>\n",
       "      <td>0.899742</td>\n",
       "      <td>0.920896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.221348</td>\n",
       "      <td>0.898477</td>\n",
       "      <td>0.929925</td>\n",
       "      <td>0.909290</td>\n",
       "      <td>0.929925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/seegull_gpt_augmentation_trained/checkpoint-21] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/seegull_gpt_augmentation_trained/checkpoint-42] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/seegull_gpt_augmentation_trained/checkpoint-63] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/seegull_gpt_augmentation_trained/checkpoint-84] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 332\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/seegull_gpt_augmentation_trained/checkpoint-105] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_bert/seegull_gpt_augmentation_trained/checkpoint-126 (score: 0.22134780883789062).\n",
      "Saving model checkpoint to model_output_bert/seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_bert/seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_bert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_bert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_bert/seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 34310.20 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 33772.90 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31931.63 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 30476.19 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_bert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 28378.76 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27341.39 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_bert/seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29226.24 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 30288.71 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /cs/student/msc/aisd/2025/bekenova/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 26986.70 examples/s]\n",
      "Map: 100%|██████████████████████| 36608/36608 [00:01<00:00, 31250.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'text': 'The commander could tell the soldier was disciplined.', 'label': 1, 'group': 'profession', 'data_name': 'MGSD', '__index_level_0__': 7380, 'input_ids': [101, 1996, 3474, 2071, 2425, 1996, 5268, 2001, 28675, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 21037.02 examples/s]\n",
      "Map: 100%|████████████████████████| 9152/9152 [00:00<00:00, 37789.98 examples/s]\n",
      "PyTorch: setting up devices\n",
      "Model Training and Evaluation/BERT_Models_Fine_Tuning2.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from validation: {'text': 'we expect that Queer people are artistic.', 'label': 0, 'group': 'Queer', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3705, 'input_ids': [101, 2057, 5987, 2008, 19483, 2111, 2024, 6018, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 36,608\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,432\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3432' max='3432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3432/3432 07:34, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>0.378198</td>\n",
       "      <td>0.792885</td>\n",
       "      <td>0.812937</td>\n",
       "      <td>0.799424</td>\n",
       "      <td>0.812937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.346956</td>\n",
       "      <td>0.822737</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.825214</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.822978</td>\n",
       "      <td>0.834852</td>\n",
       "      <td>0.828051</td>\n",
       "      <td>0.834852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.468415</td>\n",
       "      <td>0.827566</td>\n",
       "      <td>0.825896</td>\n",
       "      <td>0.826717</td>\n",
       "      <td>0.825896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.542588</td>\n",
       "      <td>0.824201</td>\n",
       "      <td>0.837982</td>\n",
       "      <td>0.829900</td>\n",
       "      <td>0.837982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.590092</td>\n",
       "      <td>0.826006</td>\n",
       "      <td>0.835319</td>\n",
       "      <td>0.830158</td>\n",
       "      <td>0.835319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-572] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716/special_tokens_map.json\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1716] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2288] due to args.save_total_limit\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: group, __index_level_0__, data_name, text. If group, __index_level_0__, data_name, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9152\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432/special_tokens_map.json\n",
      "Deleting older checkpoint [model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-2860] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-1144 (score: 0.3469562232494354).\n",
      "Deleting older checkpoint [model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/checkpoint-3432] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Configuration saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model weights saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "tokenizer config file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/tokenizer_config.json\n",
      "Special tokens file saved in model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/special_tokens_map.json\n",
      "loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 35283.48 examples/s]\n",
      "Map: 100%|██████████████████████████| 653/653 [00:00<00:00, 34169.38 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'most people who are Gay are kites', 'label': 0, 'group': 'Gay', 'data_name': 'Winoqueer - GPT Augmentation', '__index_level_0__': 3236, 'input_ids': [101, 2087, 2111, 2040, 2024, 5637, 2024, 20497, 2015, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/winoqueer_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31292.88 examples/s]\n",
      "Map: 100%|██████████████████████████| 414/414 [00:00<00:00, 31673.60 examples/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', '__index_level_0__': 840, 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/seegull_gpt_augmentation/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 28434.64 examples/s]\n",
      "Map: 100%|██████████████████████| 10374/10374 [00:00<00:00, 27442.92 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'group': 'race', 'text': 'When i see an arab, i think they are a happy.', 'label': 0, 'data_name': 'MGSD', '__index_level_0__': 17851, 'input_ids': [101, 2043, 1045, 2156, 2019, 5424, 1010, 1045, 2228, 2027, 2024, 1037, 3407, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/model.safetensors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full results to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/full_results.csv\n",
      "Saved classification report to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/mgsd/classification_report.csv\n",
      "Number of unique labels: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 29407.80 examples/s]\n",
      "Map: 100%|██████████████████████| 11441/11441 [00:00<00:00, 30577.80 examples/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/hearts_gpu/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'text': 'The Somali refugees were labeled as unacquainted by some', 'label': 0, 'group': 'Somalis', 'data_name': 'SeeGULL - GPT Augmentation', 'input_ids': [101, 1996, 16831, 8711, 2020, 12599, 2004, 14477, 2278, 16211, 18447, 2098, 2011, 2070, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Saved full results to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/full_results.csv\n",
      "Saved classification report to: result_output_bert/merged_winoqueer_seegull_gpt_augmentation_trained/merged_winoqueer_seegull_gpt_augmentation/classification_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Provide the path to the python file \n",
    "module_path = pathlib.Path(\"Model Training and Evaluation/BERT_Models_Fine_Tuning_Replication.py\")\n",
    "\n",
    "# Upload models \n",
    "spec = importlib.util.spec_from_file_location(\"hearts_models\", module_path)\n",
    "hearts_models = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(hearts_models)\n",
    "\n",
    "# Run\n",
    "hearts_models.run_full_hearts_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050039af",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 3.1:}$ Results \n",
    "\n",
    "The expected model checkpoints, tokenizer files, and configuration artefacts were successfully generated:\n",
    "\n",
    "- /model_output_albertv2/mgsd_trained/\n",
    "\n",
    "- /model_output_albertv2/winoqueer_gpt_augmentation_trained/\n",
    "\n",
    "- /model_output_albertv2/seegull_gpt_augmentation_trained/\n",
    "\n",
    "- /model_output_albertv2/merged_winoqueer_seegull_gpt_augmentation_trained/\n",
    "\n",
    "- /result_output_albertv2\n",
    "\n",
    "- /result_output_bert\n",
    "\n",
    "- /result_output_distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71e08-ace8-4abc-a068-708bd1ac92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mgsd_baseline', 'mgsd_trained', 'winoqueer_gpt_augmentation_trained', 'seegull_gpt_augmentation_trained', 'merged_winoqueer_seegull_gpt_augmentation_trained']\n"
     ]
    }
   ],
   "source": [
    "# Check the outputs directory \n",
    "output_dir = \"/tmp/HEARTS-Text-Stereotype-Detection/model_output_albertv2\"\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d0b61-95b3-419a-9b6c-551963be66aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checkpoint-1038', 'config.json', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'tokenizer.json', 'training_args.bin']\n"
     ]
    }
   ],
   "source": [
    "# Check the outputs directory for MSGD trained model \n",
    "mgsd_path = \"/tmp/HEARTS-Text-Stereotype-Detection/model_output_albertv2/mgsd_trained\"\n",
    "print(os.listdir(mgsd_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eed655-800a-49fd-8bec-4be374921421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_on</th>\n",
       "      <th>eval_on</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.829560</td>\n",
       "      <td>0.812057</td>\n",
       "      <td>0.804933</td>\n",
       "      <td>0.808239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.817525</td>\n",
       "      <td>0.798737</td>\n",
       "      <td>0.791267</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.886902</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.885452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.973671</td>\n",
       "      <td>0.974750</td>\n",
       "      <td>0.974207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.806485</td>\n",
       "      <td>0.795266</td>\n",
       "      <td>0.760456</td>\n",
       "      <td>0.772482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.802741</td>\n",
       "      <td>0.769727</td>\n",
       "      <td>0.781473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.748792</td>\n",
       "      <td>0.733152</td>\n",
       "      <td>0.759058</td>\n",
       "      <td>0.735743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>0.815491</td>\n",
       "      <td>0.611225</td>\n",
       "      <td>0.602798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.662657</td>\n",
       "      <td>0.653821</td>\n",
       "      <td>0.657215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.687199</td>\n",
       "      <td>0.647613</td>\n",
       "      <td>0.638152</td>\n",
       "      <td>0.641524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.893720</td>\n",
       "      <td>0.884995</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.878625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.777948</td>\n",
       "      <td>0.752749</td>\n",
       "      <td>0.768117</td>\n",
       "      <td>0.758341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.699851</td>\n",
       "      <td>0.660521</td>\n",
       "      <td>0.638108</td>\n",
       "      <td>0.643667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.679873</td>\n",
       "      <td>0.634276</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.615635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.756039</td>\n",
       "      <td>0.738873</td>\n",
       "      <td>0.764493</td>\n",
       "      <td>0.742479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>albertv2</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.981623</td>\n",
       "      <td>0.974701</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.979570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.842234</td>\n",
       "      <td>0.823738</td>\n",
       "      <td>0.827292</td>\n",
       "      <td>0.825447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.831213</td>\n",
       "      <td>0.811771</td>\n",
       "      <td>0.815433</td>\n",
       "      <td>0.813525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.891304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.978560</td>\n",
       "      <td>0.974890</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.975954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.826239</td>\n",
       "      <td>0.808790</td>\n",
       "      <td>0.799877</td>\n",
       "      <td>0.803933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.830923</td>\n",
       "      <td>0.814864</td>\n",
       "      <td>0.804157</td>\n",
       "      <td>0.808953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.713277</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.711166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.817764</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>0.770542</td>\n",
       "      <td>0.783431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.715497</td>\n",
       "      <td>0.682468</td>\n",
       "      <td>0.645958</td>\n",
       "      <td>0.653108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.699730</td>\n",
       "      <td>0.661440</td>\n",
       "      <td>0.625898</td>\n",
       "      <td>0.631083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.900966</td>\n",
       "      <td>0.885643</td>\n",
       "      <td>0.894928</td>\n",
       "      <td>0.889947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.848392</td>\n",
       "      <td>0.838442</td>\n",
       "      <td>0.812981</td>\n",
       "      <td>0.823317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.675203</td>\n",
       "      <td>0.648986</td>\n",
       "      <td>0.658874</td>\n",
       "      <td>0.651479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.652882</td>\n",
       "      <td>0.625725</td>\n",
       "      <td>0.633859</td>\n",
       "      <td>0.627560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.735110</td>\n",
       "      <td>0.764493</td>\n",
       "      <td>0.726933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.992343</td>\n",
       "      <td>0.988789</td>\n",
       "      <td>0.994253</td>\n",
       "      <td>0.991441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.822918</td>\n",
       "      <td>0.802137</td>\n",
       "      <td>0.808801</td>\n",
       "      <td>0.805193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.808849</td>\n",
       "      <td>0.787041</td>\n",
       "      <td>0.793631</td>\n",
       "      <td>0.790040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.917874</td>\n",
       "      <td>0.903828</td>\n",
       "      <td>0.914855</td>\n",
       "      <td>0.908891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.986217</td>\n",
       "      <td>0.983976</td>\n",
       "      <td>0.985079</td>\n",
       "      <td>0.984524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.806398</td>\n",
       "      <td>0.786350</td>\n",
       "      <td>0.776844</td>\n",
       "      <td>0.781091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.807789</td>\n",
       "      <td>0.787865</td>\n",
       "      <td>0.779667</td>\n",
       "      <td>0.783385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.765700</td>\n",
       "      <td>0.748690</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>0.752678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>mgsd_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.810107</td>\n",
       "      <td>0.839045</td>\n",
       "      <td>0.731614</td>\n",
       "      <td>0.753465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.716721</td>\n",
       "      <td>0.686388</td>\n",
       "      <td>0.641071</td>\n",
       "      <td>0.648040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.699055</td>\n",
       "      <td>0.662034</td>\n",
       "      <td>0.617653</td>\n",
       "      <td>0.621496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.896135</td>\n",
       "      <td>0.882681</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.883363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>seegull_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.883614</td>\n",
       "      <td>0.870330</td>\n",
       "      <td>0.866878</td>\n",
       "      <td>0.868559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>merged_winoqueer_seegull_gpt_augmentation</td>\n",
       "      <td>0.700551</td>\n",
       "      <td>0.661677</td>\n",
       "      <td>0.642845</td>\n",
       "      <td>0.648156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>mgsd</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.638810</td>\n",
       "      <td>0.618375</td>\n",
       "      <td>0.622647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>seegull_gpt_augmentation</td>\n",
       "      <td>0.707729</td>\n",
       "      <td>0.714491</td>\n",
       "      <td>0.740942</td>\n",
       "      <td>0.701243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>winoqueer_gpt_augmentation_trained</td>\n",
       "      <td>winoqueer_gpt_augmentation</td>\n",
       "      <td>0.977029</td>\n",
       "      <td>0.970883</td>\n",
       "      <td>0.978182</td>\n",
       "      <td>0.974379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model                                           train_on  \\\n",
       "0     albertv2  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "1     albertv2  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "2     albertv2  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "3     albertv2  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "4     albertv2                                       mgsd_trained   \n",
       "5     albertv2                                       mgsd_trained   \n",
       "6     albertv2                                       mgsd_trained   \n",
       "7     albertv2                                       mgsd_trained   \n",
       "8     albertv2                   seegull_gpt_augmentation_trained   \n",
       "9     albertv2                   seegull_gpt_augmentation_trained   \n",
       "10    albertv2                   seegull_gpt_augmentation_trained   \n",
       "11    albertv2                   seegull_gpt_augmentation_trained   \n",
       "12    albertv2                 winoqueer_gpt_augmentation_trained   \n",
       "13    albertv2                 winoqueer_gpt_augmentation_trained   \n",
       "14    albertv2                 winoqueer_gpt_augmentation_trained   \n",
       "15    albertv2                 winoqueer_gpt_augmentation_trained   \n",
       "16        bert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "17        bert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "18        bert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "19        bert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "20        bert                                       mgsd_trained   \n",
       "21        bert                                       mgsd_trained   \n",
       "22        bert                                       mgsd_trained   \n",
       "23        bert                                       mgsd_trained   \n",
       "24        bert                   seegull_gpt_augmentation_trained   \n",
       "25        bert                   seegull_gpt_augmentation_trained   \n",
       "26        bert                   seegull_gpt_augmentation_trained   \n",
       "27        bert                   seegull_gpt_augmentation_trained   \n",
       "28        bert                 winoqueer_gpt_augmentation_trained   \n",
       "29        bert                 winoqueer_gpt_augmentation_trained   \n",
       "30        bert                 winoqueer_gpt_augmentation_trained   \n",
       "31        bert                 winoqueer_gpt_augmentation_trained   \n",
       "32  distilbert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "33  distilbert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "34  distilbert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "35  distilbert  merged_winoqueer_seegull_gpt_augmentation_trained   \n",
       "36  distilbert                                       mgsd_trained   \n",
       "37  distilbert                                       mgsd_trained   \n",
       "38  distilbert                                       mgsd_trained   \n",
       "39  distilbert                                       mgsd_trained   \n",
       "40  distilbert                   seegull_gpt_augmentation_trained   \n",
       "41  distilbert                   seegull_gpt_augmentation_trained   \n",
       "42  distilbert                   seegull_gpt_augmentation_trained   \n",
       "43  distilbert                   seegull_gpt_augmentation_trained   \n",
       "44  distilbert                 winoqueer_gpt_augmentation_trained   \n",
       "45  distilbert                 winoqueer_gpt_augmentation_trained   \n",
       "46  distilbert                 winoqueer_gpt_augmentation_trained   \n",
       "47  distilbert                 winoqueer_gpt_augmentation_trained   \n",
       "\n",
       "                                      eval_on  accuracy  macro_precision  \\\n",
       "0   merged_winoqueer_seegull_gpt_augmentation  0.829560         0.812057   \n",
       "1                                        mgsd  0.817525         0.798737   \n",
       "2                    seegull_gpt_augmentation  0.898551         0.886902   \n",
       "3                  winoqueer_gpt_augmentation  0.977029         0.973671   \n",
       "4   merged_winoqueer_seegull_gpt_augmentation  0.806485         0.795266   \n",
       "5                                        mgsd  0.813187         0.802741   \n",
       "6                    seegull_gpt_augmentation  0.748792         0.733152   \n",
       "7                  winoqueer_gpt_augmentation  0.736600         0.815491   \n",
       "8   merged_winoqueer_seegull_gpt_augmentation  0.699851         0.662657   \n",
       "9                                        mgsd  0.687199         0.647613   \n",
       "10                   seegull_gpt_augmentation  0.893720         0.884995   \n",
       "11                 winoqueer_gpt_augmentation  0.777948         0.752749   \n",
       "12  merged_winoqueer_seegull_gpt_augmentation  0.699851         0.660521   \n",
       "13                                       mgsd  0.679873         0.634276   \n",
       "14                   seegull_gpt_augmentation  0.756039         0.738873   \n",
       "15                 winoqueer_gpt_augmentation  0.981623         0.974701   \n",
       "16  merged_winoqueer_seegull_gpt_augmentation  0.842234         0.823738   \n",
       "17                                       mgsd  0.831213         0.811771   \n",
       "18                   seegull_gpt_augmentation  0.903382         0.891304   \n",
       "19                 winoqueer_gpt_augmentation  0.978560         0.974890   \n",
       "20  merged_winoqueer_seegull_gpt_augmentation  0.826239         0.808790   \n",
       "21                                       mgsd  0.830923         0.814864   \n",
       "22                   seegull_gpt_augmentation  0.722222         0.713277   \n",
       "23                 winoqueer_gpt_augmentation  0.817764         0.806701   \n",
       "24  merged_winoqueer_seegull_gpt_augmentation  0.715497         0.682468   \n",
       "25                                       mgsd  0.699730         0.661440   \n",
       "26                   seegull_gpt_augmentation  0.900966         0.885643   \n",
       "27                 winoqueer_gpt_augmentation  0.848392         0.838442   \n",
       "28  merged_winoqueer_seegull_gpt_augmentation  0.675203         0.648986   \n",
       "29                                       mgsd  0.652882         0.625725   \n",
       "30                   seegull_gpt_augmentation  0.734300         0.735110   \n",
       "31                 winoqueer_gpt_augmentation  0.992343         0.988789   \n",
       "32  merged_winoqueer_seegull_gpt_augmentation  0.822918         0.802137   \n",
       "33                                       mgsd  0.808849         0.787041   \n",
       "34                   seegull_gpt_augmentation  0.917874         0.903828   \n",
       "35                 winoqueer_gpt_augmentation  0.986217         0.983976   \n",
       "36  merged_winoqueer_seegull_gpt_augmentation  0.806398         0.786350   \n",
       "37                                       mgsd  0.807789         0.787865   \n",
       "38                   seegull_gpt_augmentation  0.765700         0.748690   \n",
       "39                 winoqueer_gpt_augmentation  0.810107         0.839045   \n",
       "40  merged_winoqueer_seegull_gpt_augmentation  0.716721         0.686388   \n",
       "41                                       mgsd  0.699055         0.662034   \n",
       "42                   seegull_gpt_augmentation  0.896135         0.882681   \n",
       "43                 winoqueer_gpt_augmentation  0.883614         0.870330   \n",
       "44  merged_winoqueer_seegull_gpt_augmentation  0.700551         0.661677   \n",
       "45                                       mgsd  0.682861         0.638810   \n",
       "46                   seegull_gpt_augmentation  0.707729         0.714491   \n",
       "47                 winoqueer_gpt_augmentation  0.977029         0.970883   \n",
       "\n",
       "    macro_recall  macro_f1  \n",
       "0       0.804933  0.808239  \n",
       "1       0.791267  0.794700  \n",
       "2       0.884058  0.885452  \n",
       "3       0.974750  0.974207  \n",
       "4       0.760456  0.772482  \n",
       "5       0.769727  0.781473  \n",
       "6       0.759058  0.735743  \n",
       "7       0.611225  0.602798  \n",
       "8       0.653821  0.657215  \n",
       "9       0.638152  0.641524  \n",
       "10      0.873188  0.878625  \n",
       "11      0.768117  0.758341  \n",
       "12      0.638108  0.643667  \n",
       "13      0.611765  0.615635  \n",
       "14      0.764493  0.742479  \n",
       "15      0.985063  0.979570  \n",
       "16      0.827292  0.825447  \n",
       "17      0.815433  0.813525  \n",
       "18      0.891304  0.891304  \n",
       "19      0.977043  0.975954  \n",
       "20      0.799877  0.803933  \n",
       "21      0.804157  0.808953  \n",
       "22      0.739130  0.711166  \n",
       "23      0.770542  0.783431  \n",
       "24      0.645958  0.653108  \n",
       "25      0.625898  0.631083  \n",
       "26      0.894928  0.889947  \n",
       "27      0.812981  0.823317  \n",
       "28      0.658874  0.651479  \n",
       "29      0.633859  0.627560  \n",
       "30      0.764493  0.726933  \n",
       "31      0.994253  0.991441  \n",
       "32      0.808801  0.805193  \n",
       "33      0.793631  0.790040  \n",
       "34      0.914855  0.908891  \n",
       "35      0.985079  0.984524  \n",
       "36      0.776844  0.781091  \n",
       "37      0.779667  0.783385  \n",
       "38      0.775362  0.752678  \n",
       "39      0.731614  0.753465  \n",
       "40      0.641071  0.648040  \n",
       "41      0.617653  0.621496  \n",
       "42      0.884058  0.883363  \n",
       "43      0.866878  0.868559  \n",
       "44      0.642845  0.648156  \n",
       "45      0.618375  0.622647  \n",
       "46      0.740942  0.701243  \n",
       "47      0.978182  0.974379  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the project root\n",
    "PROJECT_ROOT = Path(\"/tmp/HEARTS-Text-Stereotype-Detection\")\n",
    "\n",
    "# Build the summary table\n",
    "summary_df = collect_summary(PROJECT_ROOT).sort_values([\"model\", \"train_on\", \"eval_on\"])\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf585a6-6fd2-4dba-97c3-ba97142a222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_short</th>\n",
       "      <th>ASeeGULL</th>\n",
       "      <th>AWinQ</th>\n",
       "      <th>EMGSD</th>\n",
       "      <th>MGSD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>train_short</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">albertv2</th>\n",
       "      <th>ASeeGULL</th>\n",
       "      <td>0.878625</td>\n",
       "      <td>0.758341</td>\n",
       "      <td>0.657215</td>\n",
       "      <td>0.641524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWinQ</th>\n",
       "      <td>0.742479</td>\n",
       "      <td>0.979570</td>\n",
       "      <td>0.643667</td>\n",
       "      <td>0.615635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMGSD</th>\n",
       "      <td>0.885452</td>\n",
       "      <td>0.974207</td>\n",
       "      <td>0.808239</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGSD</th>\n",
       "      <td>0.735743</td>\n",
       "      <td>0.602798</td>\n",
       "      <td>0.772482</td>\n",
       "      <td>0.781473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">bert</th>\n",
       "      <th>ASeeGULL</th>\n",
       "      <td>0.889947</td>\n",
       "      <td>0.823317</td>\n",
       "      <td>0.653108</td>\n",
       "      <td>0.631083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWinQ</th>\n",
       "      <td>0.726933</td>\n",
       "      <td>0.991441</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>0.627560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMGSD</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.975954</td>\n",
       "      <td>0.825447</td>\n",
       "      <td>0.813525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGSD</th>\n",
       "      <td>0.711166</td>\n",
       "      <td>0.783431</td>\n",
       "      <td>0.803933</td>\n",
       "      <td>0.808953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">distilbert</th>\n",
       "      <th>ASeeGULL</th>\n",
       "      <td>0.883363</td>\n",
       "      <td>0.868559</td>\n",
       "      <td>0.648040</td>\n",
       "      <td>0.621496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWinQ</th>\n",
       "      <td>0.701243</td>\n",
       "      <td>0.974379</td>\n",
       "      <td>0.648156</td>\n",
       "      <td>0.622647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMGSD</th>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.984524</td>\n",
       "      <td>0.805193</td>\n",
       "      <td>0.790040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGSD</th>\n",
       "      <td>0.752678</td>\n",
       "      <td>0.753465</td>\n",
       "      <td>0.781091</td>\n",
       "      <td>0.783385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "eval_short              ASeeGULL     AWinQ     EMGSD      MGSD\n",
       "model      train_short                                        \n",
       "albertv2   ASeeGULL     0.878625  0.758341  0.657215  0.641524\n",
       "           AWinQ        0.742479  0.979570  0.643667  0.615635\n",
       "           EMGSD        0.885452  0.974207  0.808239  0.794700\n",
       "           MGSD         0.735743  0.602798  0.772482  0.781473\n",
       "bert       ASeeGULL     0.889947  0.823317  0.653108  0.631083\n",
       "           AWinQ        0.726933  0.991441  0.651479  0.627560\n",
       "           EMGSD        0.891304  0.975954  0.825447  0.813525\n",
       "           MGSD         0.711166  0.783431  0.803933  0.808953\n",
       "distilbert ASeeGULL     0.883363  0.868559  0.648040  0.621496\n",
       "           AWinQ        0.701243  0.974379  0.648156  0.622647\n",
       "           EMGSD        0.908891  0.984524  0.805193  0.790040\n",
       "           MGSD         0.752678  0.753465  0.781091  0.783385"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping from full training directory names to short labels\n",
    "train_map = {\n",
    "    \"mgsd_trained\": \"MGSD\",\n",
    "    \"winoqueer_gpt_augmentation_trained\": \"AWinQ\",\n",
    "    \"seegull_gpt_augmentation_trained\": \"ASeeGULL\",\n",
    "    \"merged_winoqueer_seegull_gpt_augmentation_trained\": \"EMGSD\",\n",
    "}\n",
    "\n",
    "# Mapping from full evaluation directory names to short labels\n",
    "eval_map = {\n",
    "    \"mgsd\": \"MGSD\",\n",
    "    \"winoqueer_gpt_augmentation\": \"AWinQ\",\n",
    "    \"seegull_gpt_augmentation\": \"ASeeGULL\",\n",
    "    \"merged_winoqueer_seegull_gpt_augmentation\": \"EMGSD\",\n",
    "}\n",
    "\n",
    "# Build and display the table\n",
    "table = build_table(summary_df)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ededd",
   "metadata": {},
   "source": [
    "### Table: HEARTS Replicated Results\n",
    "\n",
    "The table below summarises the replicated Macro F1 scores for ALBERT-V2, BERT, and DistilBERT across all combinations of training and evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000719af",
   "metadata": {},
   "source": [
    "![HEARTS Replicated Results](COMP0173_Figures/hearts_replicated_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb5c50",
   "metadata": {},
   "source": [
    "### Table: HEARTS Original Results [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338501c",
   "metadata": {},
   "source": [
    "![Model Results Table](COMP0173_Figures/hearts_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08950f",
   "metadata": {},
   "source": [
    "### Results Interpretation\n",
    "\n",
    "The replicated Macro F1 scores for ALBERT-V2, BERT, and DistilBERT closely match the results reported in the HEARTS paper, with a few exceptions. For nearly all train-test combinations, the reproduced valuesexcept for two; fall within the required $±5\\%$ range, confirming the successful replication of the baseline AI methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca732352",
   "metadata": {},
   "source": [
    "#### ALBERT-V2\n",
    "\n",
    "For ALBERT-V2, the replicated Macro F1 scores generally align with the original results. Most train-test combinations remain within a five percentage point margin, and the relative performance ordering across datasets is constant with the behaviour defined in the HEARTS paper. Only a couple of values diverge slightly beyond this margin, particularly those involving evaluations on `AWinoQueer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad83df7",
   "metadata": {},
   "source": [
    "![ALBERT-V2 Results Table](COMP0173_Figures/results_comparison_albert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1898b8",
   "metadata": {},
   "source": [
    "#### BERT\n",
    "\n",
    "The BERT model shows remarkable reproducibility. All replicated scores are consistently within a $±5\\%$ range, with differences between the original and replicated values typically being minor, often just one or two points apart. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5a28f",
   "metadata": {},
   "source": [
    "![BERT Results Table](COMP0173_Figures/results_comparison_bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7af00",
   "metadata": {},
   "source": [
    "#### DistilBERT\n",
    "\n",
    "DistilBERT demonstrates a strong alignment between the original and replicated metrics. Every train-test result falls within the target deviation range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a409f",
   "metadata": {},
   "source": [
    "![DistilBERT-V2 Results Table](COMP0173_Figures/results_comparison_distilbert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad2c76",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[1] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS: A holistic framework for explainable, sustainable and robust text stereotype detection.\n",
    "arXiv preprint arXiv:2409.11579.\n",
    "Available at: https://arxiv.org/abs/2409.11579\n",
    "(Accessed: 4 December 2025).\n",
    "https://doi.org/10.48550/arXiv.2409.11579\n",
    "\n",
    "[2] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS-Text-Stereotype-Detection (GitHub Repository).\n",
    "Available at: https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[3] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. Holistic AI. 2024.\n",
    "EMGSD: Expanded Multi-Group Stereotype Dataset (HuggingFace Dataset).\n",
    "Available at: https://huggingface.co/datasets/holistic-ai/EMGSD\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[4] University College London Technical Support Group (TSG).\n",
    "2025. GPU Access and Usage Documentation.\n",
    "Available at: https://tsg.cs.ucl.ac.uk/gpus/\n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[5] United Nations. 2025. The 2030 Agenda for Sustainable Development. \n",
    "Available at: https://sdgs.un.org/2030agenda \n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[6] Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, and Ekaterina Artemova. 2024.\n",
    "RuBia: A Russian Language Bias Detection Dataset.\n",
    "Available at: https://arxiv.org/abs/2403.17553\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[7] Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, and Ekaterina Artemova. 2024.\n",
    "RuBia-Dataset (GitHub Repository).\n",
    "Available at: https://github.com/vergrig/RuBia-Dataset\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[8] Sismetanin. 2020. Toxic Comments Detection in Russian (GitHub Repository).\n",
    "Available at: https://github.com/sismetanin/toxic-comments-detection-in-russian\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[9] DeepPavlov. 2019. RuBERT-base-cased (Hugging Face Model).\n",
    "Available at: https://huggingface.co/DeepPavlov/rubert-base-cased\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[10] AI-Forever. 2023. RuBERT-base (Hugging Face Model).\n",
    "Available at: https://huggingface.co/ai-forever/ruBert-base\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[11] Hugging Face. 2024. XLM-RoBERTa: Model Documentation.\n",
    "Available at: https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta\n",
    "(Accessed: 9 December 2025).\n",
    "\n",
    "[12] DeepPavlov. 2020. ruBERT-base-cased-sentence (Hugging Face Model).\n",
    "Available at: https://huggingface.co/DeepPavlov/rubert-base-cased-sentence\n",
    "(Accessed: 9 December 2025)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
