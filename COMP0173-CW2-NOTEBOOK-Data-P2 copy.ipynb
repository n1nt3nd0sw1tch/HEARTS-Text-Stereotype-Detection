{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c925ac8a",
   "metadata": {},
   "source": [
    "# COMP0173: Coursework 2\n",
    "\n",
    "The paper HEARTS: A Holistic Framework for Explainable, Sustainable, and Robust Text Stereotype Detection by Theo King, Zekun Wu et al. (2024) presents a comprehensive approach to analysing and detecting stereotypes in text [1]. The authors introduce the HEARTS framework, which integrates model explainability, carbon-efficient training, and accurate evaluation across multiple bias-sensitive datasets. By using transformer-based models such as ALBERT-V2, BERT, and DistilBERT, this research project demonstrates that stereotype detection performance varies significantly across dataset sources, underlining the need for diverse evaluation benchmarks. The paper provides publicly available datasets and code [2], allowing full reproducibility and offering a standardised methodology for future research on bias and stereotype detection in Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068fd33",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "All figures produced during this notebook are stored in the project’s `/COMP0173_Figures` directory.\n",
    "The corresponding LaTeX-formatted performance comparison tables, including ALBERT-V2, BERT, and DistilBERT are stored in `/COMP0173_PDF`, with the compiled document available as `COMP0173-CW2-TABLES.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279720f4",
   "metadata": {},
   "source": [
    "# Technical Implementation (70%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install -r requirements.txt\n",
    "# pip install transformers\n",
    "# pip install --upgrade transformers\n",
    "# pip install --upgrade tokenizers\n",
    "# pip install -U sentence-transformers\n",
    "# pip install natasha\n",
    "# pip install datasets\n",
    "# pip install --user -U nltk\n",
    "# conda install -c anaconda nltk\n",
    "# pip install --upgrade openai pandas tqdm\n",
    "# pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_trf\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download ru_core_news_lg\n",
    "\n",
    "# # GPU\n",
    "# pip install -U 'spacy[cuda12x]'\n",
    "# # GPU - Train Models\n",
    "# pip install -U 'spacy[cuda12x,transformers,lookups]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import random, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(color_codes=True)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"pkg_resources is deprecated as an API\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526aa252-67bc-4800-a639-661ef2b90b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib.util, pathlib\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "from importlib import reload\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import difflib\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b7bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hearts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM, XLMWithLMHeadModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import platform\n",
    "from datasets import load_dataset\n",
    "import spacy \n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import natasha\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the GPU host (UCL access)\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# # Path\n",
    "# import os\n",
    "# os.chdir(\"/tmp/HEARTS-Text-Stereotype-Detection\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ddc52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part 2: Identify a contextually relevant challenge in your country or region of your choice that can be addressed using the same AI approach\n",
    "\n",
    "**Content Warning:**\n",
    "This notebook contains examples of stereotypes and anti-stereotypes that\n",
    "may be offensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798713bb",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Problem and SDG alignment\n",
    "\n",
    "This coursework supports Sustainable Development Goal (SDG) 5: Gender Equality - *Achieve gender equality and empower all women and girls*, SDG 9: Industry, Innovation, and Infrastructure - *Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation*, SDG 10: Reduced Inequalities - *Reduce inequality within and among countries*, and SDG 16: Peace, Justice, and Strong Institutions: - *Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels* [5].\n",
    "\n",
    "The specific targets covered by this coursework are:\n",
    "\n",
    "- SDG 5.1: *End all forms of discrimination against all women and girls everywhere*\n",
    "\n",
    "- SDG 5.b: *Enhance the use of enabling technology, in particular information and communications technology, to promote the empowerment of women*\n",
    "\n",
    "- SDG 10.2: *By 2030, empower and promote the social, economic and political inclusion of all, irrespective of age, sex, disability, race, ethnicity, origin, religion or economic or other status*\n",
    "\n",
    "- SDG 10.3: *Ensure equal opportunity and reduce inequalities of outcome, including by eliminating discriminatory laws, policies and practices and promoting appropriate legislation, policies and action in this regard*\n",
    "\n",
    "- SDG 16.1: *Significantly reduce all forms of violence and related death rates everywhere*\n",
    "\n",
    "- SDG 16.6: *Develop effective, accountable and transparent institutions at all levels*\n",
    "\n",
    "- SDG 16.10: *Ensure public access to information and protect fundamental freedoms, in accordance with national legislation and international agreements*\n",
    "\n",
    "- SDG 16.b: *Promote and enforce non-discriminatory laws and policies for sustainable development*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf01c4",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Limitations and ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f71d69",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 3:}$ Scalability and sustainability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00197cf",
   "metadata": {},
   "source": [
    "## Part 3: Curate or identify an alternative dataset appropriate for your context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ef1ca",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 1:}$ Identify contextually appropriate dataset\n",
    "\n",
    "1. RuBias\n",
    "2. Kaggle\n",
    "3. RuHateBe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b59f2",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 2:}$ Document data collection/access process and ethical considerations\n",
    "\n",
    "Mention where you got these datasets - provide refs and what should be cleaned from these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b406ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in its raw format\n",
    "# RuBias\n",
    "rubias = pd.read_csv(\"COMP0173_Data/rubias.tsv\", sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "rubias = rubias.rename(columns={\"domain\": \"stereotype_type\"})\n",
    "\n",
    "# Change the level name\n",
    "rubias[\"stereotype_type\"] = rubias[\"stereotype_type\"].replace(\"class\", \"profession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in its raw format\n",
    "# RuSter\n",
    "ruster = pd.read_json(\"COMP0173_Stereotypes/stereotypes.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "ruster.to_csv(\"COMP0173_Data/ruster.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdee8e",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart_domain(df, column, name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot the percentage distribution of social-group domains as a styled pie chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing a categorical column representing social domains.\n",
    "    column : str, optional\n",
    "        Name of the column in `df` holding domain labels. \n",
    "        \n",
    "    column : str, optional\n",
    "        Name of the dataset. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a pie chart visualising the proportional distribution of categories.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The function applies a custom colour palette tailored for the RuBias dataset \n",
    "    (gender, class, nationality, LGBTQ). Any unseen categories default to grey.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute relative frequency (%) of categories\n",
    "    domain_counts = df[column].value_counts(normalize=True) * 100\n",
    "    labels = domain_counts.index\n",
    "    sizes = domain_counts.values\n",
    "\n",
    "    # Predefined colour palette\n",
    "    color_map = {\n",
    "        'gender':      \"#CA5353\",  \n",
    "        'profession':  \"#F1A72F\",  \n",
    "        'nationality': \"#559A67\",  \n",
    "        'lgbtq':       \"#527BCD\",  \n",
    "    }\n",
    "    # Assign colours; fallback to grey for unknown labels\n",
    "    colors = [color_map.get(lbl, 'grey') for lbl in labels]\n",
    "\n",
    "    # Create compact, high-resolution figure\n",
    "    plt.figure(figsize=(5.5, 4), dpi=155)\n",
    "\n",
    "    # Draw pie chart with formatted percentages\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        sizes,\n",
    "        labels=None,\n",
    "        autopct='%1.1f%%',\n",
    "        pctdistance=0.55,\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        wedgeprops={'linewidth': 2, 'edgecolor': 'white'}\n",
    "    )\n",
    "\n",
    "    # Style displayed percentage numbers\n",
    "    for t in autotexts:\n",
    "        t.set_fontsize(10)\n",
    "        t.set_color(\"black\")\n",
    "\n",
    "    # Title\n",
    "    plt.title(f\"Social Group Distribution: {name}\", fontsize=16)\n",
    "\n",
    "    # Legend placed to the right of the figure\n",
    "    plt.legend(\n",
    "        wedges,\n",
    "        labels,\n",
    "        title=\"Domain\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        fontsize=11,\n",
    "        title_fontsize=12\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string(texts: pd.Series) -> pd.Series:\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalise Russian stereotype strings.\n",
    "\n",
    "    Operations\n",
    "    ----------\n",
    "    - lowercase\n",
    "    - remove punctuation (except comma, hyphen, underscore)\n",
    "    - replace '-' and '—' with spaces\n",
    "    - collapse multiple spaces\n",
    "    - normalise 'ё' → 'е'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : pd.Series\n",
    "        Series of raw text strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Normalised text strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep comma, hyphen, underscore\n",
    "    punc = ''.join(ch for ch in string.punctuation if ch not in ',-_')\n",
    "\n",
    "    trans_table = str.maketrans('-—', '  ', punc)\n",
    "\n",
    "    def _norm(s: str) -> str:\n",
    "        s = str(s).lower().translate(trans_table)\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace('ё', 'е')\n",
    "        return s\n",
    "\n",
    "    return texts.apply(_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess the RUBIAS dataset into a unified stereotype format.\n",
    "\n",
    "    Removes index-like columns, anti-trope content and irrelevant\n",
    "    task types, standardises column names and stereotype-type labels,\n",
    "    cleans the text field, and removes empty/duplicate rows.\n",
    "\n",
    "    Output schema:\n",
    "        * text\n",
    "        * category          (fixed to 'stereotype')\n",
    "        * stereotype_type   (e.g. gender, profession, nationality)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw RUBIAS dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe ready for manual curation or augmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop any index-like columns such as 'Unnamed: 0'\n",
    "    unnamed_cols = [c for c in df.columns if c.startswith(\"Unnamed\")]\n",
    "    if unnamed_cols:\n",
    "        df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "    # Remove anti-stereotype variants\n",
    "    if \"anti-trope\" in df.columns:\n",
    "        df = df.drop(columns=[\"anti-trope\"])\n",
    "\n",
    "    # Remove non-relevant generation templates\n",
    "    irrelevant = {\"template_hetpos\", \"freeform_repres\"}\n",
    "    if \"task_type\" in df.columns:\n",
    "        df = df[~df[\"task_type\"].isin(irrelevant)]\n",
    "        df = df.drop(columns=[\"task_type\"])\n",
    "\n",
    "    # Standardise schema\n",
    "    df = df.rename(columns={\"pro-trope\": \"text\"})\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df = df[[\"text\", \"stereotype_type\"]]\n",
    "\n",
    "    # Assign fixed category label\n",
    "    df[\"category\"] = \"stereotype\"\n",
    "\n",
    "    # Format strings\n",
    "    df[\"text\"] = format_string(df[\"text\"])\n",
    "\n",
    "    # Optional: drop duplicates and empties \n",
    "    df = df[df[\"text\"].notna() & (df[\"text\"].str.len() > 0)]\n",
    "    df = df.drop_duplicates(subset=\"text\")\n",
    "\n",
    "    # Order columns\n",
    "    df = df[[\"text\", \"category\", \"stereotype_type\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_semantic_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"text\",\n",
    "    group_col: str = \"stereotype_type\",\n",
    "    model_name: str = \"DeepPavlov/rubert-base-cased-sentence\",\n",
    "    border_sim: float = 0.98,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove semantically near-duplicate text entries from a dataframe.\n",
    "\n",
    "    This function computes sentence embeddings using a SentenceTransformer\n",
    "    model and identifies near-duplicate sentences based on cosine similarity.\n",
    "    Only sentences belonging to the same group (e.g., same stereotype type)\n",
    "    are compared. For each pair of sentences that exceed the similarity \n",
    "    threshold, the later-indexed entry is removed. Detected duplicates \n",
    "    are printed to stdout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing at least the text column and optionally a \n",
    "        grouping column.\n",
    "    text_col : str, default \"text\"\n",
    "        Name of the column containing raw text to evaluate for duplicates.\n",
    "    group_col : str, default \"stereotype_type\"\n",
    "        Column name determining groups within which similarity comparisons \n",
    "        are performed. Sentences from different groups are never compared.\n",
    "    model_name : str, default \"DeepPavlov/rubert-base-cased-sentence\"\n",
    "        Identifier of a SentenceTransformer model used to compute embeddings.\n",
    "    border_sim : float, default 0.98\n",
    "        Cosine similarity threshold above which two sentences are considered\n",
    "        near-duplicates. Must be in the range [0, 1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A cleaned dataframe with near-duplicate rows removed and the index\n",
    "        reset.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function prints each detected near-duplicate pair, including the\n",
    "      kept sentence, removed sentence, and similarity score.\n",
    "    - Duplicate detection is greedy: the earliest occurrence is preserved,\n",
    "      and any later duplicates are removed.\n",
    "    - Performance may degrade for very large datasets due to O(n^2)\n",
    "      pairwise similarity comparisons.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_clean = drop_semantic_duplicates(\n",
    "    ...     df,\n",
    "    ...     text_col=\"text\",\n",
    "    ...     group_col=\"stereotype_type\",\n",
    "    ...     border_sim=0.90,\n",
    "    ... )\n",
    "    >>> df_clean.head()\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=True).copy()\n",
    "\n",
    "    sent_encoder = SentenceTransformer(model_name)\n",
    "    texts = df[text_col].tolist()\n",
    "    embeddings = sent_encoder.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    to_remove = set()\n",
    "    n = len(df)\n",
    "\n",
    "    for i in range(n):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i + 1, n):\n",
    "            if j in to_remove:\n",
    "                continue\n",
    "\n",
    "            if df.loc[i, group_col] != df.loc[j, group_col]:\n",
    "                continue\n",
    "\n",
    "            sim = util.pytorch_cos_sim(embeddings[i], embeddings[j]).item()\n",
    "\n",
    "            if sim > border_sim:\n",
    "                print(\"-\" * 80)\n",
    "                print(f\"Duplicates Found (Similarity = {sim:.3f})\")\n",
    "                print(f\"Saved [{i}]: {df.loc[i, text_col]}\")\n",
    "                print(f\"Removed [{j}]: {df.loc[j, text_col]}\")\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "                to_remove.add(j)\n",
    "\n",
    "    print(f\"\\nTotal near-duplicates removed: {len(to_remove)}\\n\")\n",
    "\n",
    "    return df.drop(index=list(to_remove)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6399050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence_claude(sentence: str,\n",
    "                            stereotype_type: str,\n",
    "                            temperature: float = 0.5) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate neutral and unrelated (nonsensical) augmentations for a given\n",
    "    Russian stereotype sentence using the Bedrock Proxy API.\n",
    "\n",
    "    This function embeds the entire instruction prompt and examples inside\n",
    "    a single user message, because the proxy does not support the `system`\n",
    "    role. The output is validated via a strict JSON schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The original stereotype sentence in Russian.\n",
    "    stereotype_type : str\n",
    "        The associated stereotype group (e.g., 'gender', 'profession').\n",
    "    temperature : float, optional\n",
    "        Sampling temperature for the LLM. Default is 0.7.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "            - 'neutral': str\n",
    "                A neutralised version of the input sentence.\n",
    "            - 'unrelated': str\n",
    "                A nonsensical, unrelated version of the input sentence.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the API returns a non-200 status code.\n",
    "    ValueError\n",
    "        If JSON parsing fails or required keys are missing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build full user prompt: instructions + examples + concrete task\n",
    "    user_content = (\n",
    "        SYSTEM_PROMPT_RU.strip()\n",
    "        + \"\\n\\nТеперь задача для конкретного примера.\\n\"\n",
    "        + \"Исходное стереотипное предложение:\\n\"\n",
    "        + f\"\\\"{sentence}\\\"\\n\\n\"\n",
    "        + f\"Тип стереотипа: {stereotype_type}\\n\\n\"\n",
    "        + \"Сгенерируй нейтральную и несвязанную версии. \"\n",
    "          \"Верни ТОЛЬКО JSON в формате:\\n\"\n",
    "          \"{ \\\"neutral\\\": \\\"...\\\", \\\"unrelated\\\": \\\"...\\\" }\"\n",
    "    )\n",
    "\n",
    "    # Message container for API\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_content,\n",
    "    }]\n",
    "\n",
    "    # Request payload (API requires team_id, api_token, model inside JSON)\n",
    "    payload = {\n",
    "        \"team_id\": TEAM_ID,\n",
    "        \"api_token\": API_TOKEN,\n",
    "        \"model\": MODEL_ID,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": temperature,\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"rubist_augmentation\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": AUG_SCHEMA,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Execute POST request\n",
    "    response = requests.post(\n",
    "        API_ENDPOINT,\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Team-ID\": TEAM_ID,\n",
    "            \"X-API-Token\": API_TOKEN,\n",
    "        },\n",
    "        json=payload,\n",
    "        timeout=60,\n",
    "    )\n",
    "\n",
    "    # Validate HTTP layer\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(\n",
    "            f\"API error {response.status_code}: {response.text[:500]}\"\n",
    "        )\n",
    "\n",
    "    # Parse API response\n",
    "    result = response.json()\n",
    "\n",
    "    # Quota reporting \n",
    "    if \"metadata\" in result and \"remaining_quota\" in result[\"metadata\"]:\n",
    "        quota = result[\"metadata\"][\"remaining_quota\"]\n",
    "        print(\n",
    "            f\"[Quota] LLM={quota['llm_cost']} | GPU={quota['gpu_cost']} | \"\n",
    "            f\"Used={quota['total_cost']}/{quota['budget_limit']} | \"\n",
    "            f\"Remaining={quota['remaining_budget']} | \"\n",
    "            f\"Usage={quota['budget_usage_percent']}%\"\n",
    "        )\n",
    "\n",
    "    # Extract model-generated JSON text\n",
    "    try:\n",
    "        raw_text = result[\"content\"][0][\"text\"]\n",
    "    except Exception as exc:\n",
    "        raise ValueError(f\"Malformed response structure: {result}\") from exc\n",
    "\n",
    "    # Parse JSON output from the model\n",
    "    try:\n",
    "        data = json.loads(raw_text)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\n",
    "            f\"Could not parse JSON from model output:\\n{raw_text}\"\n",
    "        )\n",
    "\n",
    "    # Validate required keys\n",
    "    if not all(k in data for k in (\"neutral\", \"unrelated\")):\n",
    "        raise ValueError(f\"Missing required keys in response: {data}\")\n",
    "\n",
    "    # Clean and return output\n",
    "    return {\n",
    "        \"neutral\": data[\"neutral\"].strip(),\n",
    "        \"unrelated\": data[\"unrelated\"].strip(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a0792",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape\n",
    "print(rubias.columns)\n",
    "print(rubias.shape)\n",
    "\n",
    "rubias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape\n",
    "print(ruster.columns)\n",
    "print(ruster.shape)\n",
    "\n",
    "ruster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69543f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the general information and variable type of the dataset\n",
    "rubias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5095a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the general information and variable type of the dataset\n",
    "ruster.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of unique rows\n",
    "rubias.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41350ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of unique rows\n",
    "ruster.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique domains \n",
    "print(\"Unique stereotype types: RuBias\")\n",
    "print(rubias['stereotype_type'].unique())\n",
    "\n",
    "# Print unique domains \n",
    "print(\"Unique subdomains:\")\n",
    "print(rubias['task_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3eef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique domains \n",
    "print(\"Unique stereotype types: RuSter\")\n",
    "print(ruster['stereotype_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pie chart \n",
    "pie_chart_domain(rubias, column='stereotype_type', name = \"RuBias (Original, 2221 samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pie chart \n",
    "pie_chart_domain(ruster, column='stereotype_type', name = \"RuSter (Original, 408 samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ef16d",
   "metadata": {},
   "source": [
    "### $\\color{pink}{Question\\ 3:}$ Provide data preprocessing pipeline\n",
    "\n",
    "These task_types were removed from the model as well as all anti-tropes since they do not contribute to the stereotype dataset:\n",
    "\n",
    "1. `template_hetpos`\n",
    "\n",
    "    1.1 pro-tope:\n",
    "    - Russian: Игорь любит Аню, потому что она красивая\n",
    "    - English: Igor loves Anya because she is beautiful\n",
    "\n",
    "    1.2 anti-trope: \n",
    "    - Russian: Аня любит Игоря, потому что он красивый\n",
    "    - English: Anya loves Igor because he is handsome\n",
    "\n",
    "2. `freeform_repres`\n",
    "\n",
    "    2.1 pro-tope:\n",
    "    - Russian: Он пригласил ее на романтический ужин\n",
    "    - English: He invited her to a romantic dinner\n",
    "\n",
    "    2.2 anti-trope: \n",
    "    - Russian: Она пригласила ее на романтический ужин\n",
    "    - English: She invited her to a romantic dinner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56961a1c",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and clean\n",
    "rubias_prep = data_prep(rubias)\n",
    "\n",
    "# Print the shape\n",
    "print(rubias_prep.columns)\n",
    "print(rubias_prep.shape)\n",
    "\n",
    "rubias_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f91570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save for manual cleaning \n",
    "# rubias_prep.to_excel(\"COMP0173_Temp_Data/rubias_prep.xlsx\", index=False)\n",
    "# print(\"Converted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format strings\n",
    "ruster[\"text\"] = format_string(ruster[\"text\"])\n",
    "ruster = ruster.drop(columns=[\"notes\"])\n",
    "\n",
    "# Print the shape\n",
    "print(ruster.columns)\n",
    "print(ruster.shape)\n",
    "\n",
    "ruster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11076e",
   "metadata": {},
   "source": [
    "#### Manual Cleaning \n",
    "\n",
    "1. Since the original dataset was mostly about the biases and the texts are generated by using the \"she\" and \"he\" pronouns - I will replace these by \"Woman\", \"Man\", and drop the irrelevant stereotypes than are either not common stereotypes or counterfactual, over negative, duplicates - replace common slur words in russian to more formal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7648f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the manually cleaned dataset\n",
    "rubias_manual = pd.read_excel(\"COMP0173_Temp_Data/rubias_manual.xlsx\")\n",
    "rubias_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset by merging \n",
    "rubist = pd.concat([rubias_manual, ruster], ignore_index=True)\n",
    "\n",
    "# Drop duplicates\n",
    "rubist = rubist.drop_duplicates(subset=\"text\")\n",
    "rubist = rubist.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bace7c6",
   "metadata": {},
   "source": [
    "#### RuBiST - New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0582403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop examples that are similar to others\n",
    "print(\"RuBiSt Dataset Shape - Before:\", rubist.shape)\n",
    "\n",
    "rubist_dedup = drop_semantic_duplicates(\n",
    "    rubist,\n",
    "    text_col=\"text\",\n",
    "    group_col=\"stereotype_type\",\n",
    "    border_sim=0.85,\n",
    ")\n",
    "\n",
    "print(\"RuBiSt Dataset Shape - After:\", rubist_dedup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the general information and variable type of the dataset\n",
    "rubist_dedup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pie chart \n",
    "pie_chart_domain(rubist_dedup, column='stereotype_type', name = \"RuBiSt (RuBias + RuSter, 977 samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdfc76c",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "TEAM_ID = os.getenv(\"BEDROCK_TEAM_ID\")\n",
    "API_TOKEN = os.getenv(\"BEDROCK_API_TOKEN\")\n",
    "\n",
    "API_ENDPOINT = \"https://ctwa92wg1b.execute-api.us-east-1.amazonaws.com/prod/invoke\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "# JSON\n",
    "AUG_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"neutral\":   {\"type\": \"string\"},\n",
    "        \"unrelated\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"neutral\", \"unrelated\"]\n",
    "}\n",
    "\n",
    "with open(\"COMP0173_Prompts/prompt.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "SYSTEM_PROMPT_RU = CONFIG[\"instructions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure original rows have a label_level column\n",
    "if \"label_level\" not in rubist_dedup.columns:\n",
    "    rubist_dedup[\"label_level\"] = \"stereotype\"\n",
    "\n",
    "augmented_rows = []\n",
    "\n",
    "# Iterate through all selected rows\n",
    "for _, row in tqdm(rubist_dedup.iterrows(), total=len(rubist_dedup)):\n",
    "    original_text = row[\"text\"]\n",
    "    stype = row[\"stereotype_type\"]\n",
    "\n",
    "    # Store the original stereotype row\n",
    "    stereo_row = row.copy()\n",
    "    stereo_row[\"label_level\"] = \"stereotype\"\n",
    "    augmented_rows.append(stereo_row)\n",
    "\n",
    "    # Call the augmentation API\n",
    "    try:\n",
    "        aug = augment_sentence_claude(original_text, stype)\n",
    "    except Exception as e:\n",
    "        print(\"\\nError while processing example:\")\n",
    "        print(original_text)\n",
    "        print(\"Cause:\", e)\n",
    "        continue\n",
    "\n",
    "    # Neutral version\n",
    "    neutral_row = row.copy()\n",
    "    neutral_row[\"text\"] = aug[\"neutral\"]\n",
    "    neutral_row[\"label_level\"] = \"neutral\"\n",
    "    augmented_rows.append(neutral_row)\n",
    "\n",
    "    # Unrelated version\n",
    "    unrelated_row = row.copy()\n",
    "    unrelated_row[\"text\"] = aug[\"unrelated\"]\n",
    "    unrelated_row[\"label_level\"] = \"unrelated\"\n",
    "    augmented_rows.append(unrelated_row)\n",
    "    \n",
    "    \n",
    "# Build final augmented DataFrame\n",
    "rubist_aug = pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Save final file\n",
    "rubist_aug.to_csv(\"COMP0173_Temp_Data/rubist_aug_ch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad2c76",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[1] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS: A holistic framework for explainable, sustainable and robust text stereotype detection.\n",
    "arXiv preprint arXiv:2409.11579.\n",
    "Available at: https://arxiv.org/abs/2409.11579\n",
    "(Accessed: 4 December 2025).\n",
    "https://doi.org/10.48550/arXiv.2409.11579\n",
    "\n",
    "[2] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. 2024.\n",
    "HEARTS-Text-Stereotype-Detection (GitHub Repository).\n",
    "Available at: https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[3] Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, and Philip Treleaven. Holistic AI. 2024.\n",
    "EMGSD: Expanded Multi-Group Stereotype Dataset (HuggingFace Dataset).\n",
    "Available at: https://huggingface.co/datasets/holistic-ai/EMGSD\n",
    "(Accessed: 4 December 2025).\n",
    "\n",
    "[4] University College London Technical Support Group (TSG).\n",
    "2025. GPU Access and Usage Documentation.\n",
    "Available at: https://tsg.cs.ucl.ac.uk/gpus/\n",
    "(Accessed: 6 December 2025).\n",
    "\n",
    "[5] United Nations. 2025. The 2030 Agenda for Sustainable Development. \n",
    "Available at: https://sdgs.un.org/2030agenda \n",
    "(Accessed: 6 December 2025)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
